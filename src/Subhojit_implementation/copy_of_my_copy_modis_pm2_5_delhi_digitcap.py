# -*- coding: utf-8 -*-
"""Copy of My_copy_MODIS_PM2.5_Delhi_Digitcap.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HfiSnUAaT3Mex6JMkNC05F2dB7OxbPRu

## First we will install the `necessary packages`.
"""

!pip install basemap
!pip install matplotlib_scalebar mplleaflet pykrige geopandas

from numpy import genfromtxt
import pandas as pd
import numpy as np
import geopandas as gpd
from mpl_toolkits.basemap import Basemap as Basemap
import random
from sklearn.metrics import mean_absolute_error, mean_squared_error
import pandas as pd
from scipy.spatial import cKDTree

"""## Mounting the drive"""

from google.colab import drive
drive.mount('/content/drive')

# @title Data_load
stn = pd.read_csv('/content/drive/MyDrive/Modis/stn_extrafeat.csv', delimiter=',')
totaldata=np.load("/content/drive/MyDrive/Modis/3_hr_data_Sept_2023_v1.npy",allow_pickle=True)

# wswd=np.load("/content/drive/MyDrive/Modis/DelhiData_3hr_sample_wswd.npy")

"""
Here the total data is in the form of (T, S, F) = (time, stations, features)
where T is the number of time steps.
S is the number of stations, and F is the number of features.
And below as you can see we are accessing all the features.

"""

totaldata=np.float32(totaldata)

# planetary boundry layer height
pbl=totaldata[:,:,11]

# Relative humidity  for all the time steps and for all the stations.
rh=totaldata[:,:,4]

# Temperature for all the time steps and for all the stations.
temp=totaldata[:,:,7]

# pm10 particulate matter value for all the time steps and for all the stations.
pm10=totaldata[:,:,-1]

totaldata=np.delete(totaldata, [1,2,3,8,14,15],2)

# 0    1   2   3  4    5   6   7   8     9   10 11   12       13   14   15 #pm25 no2 SO2 nh3 rh ws wd at prec v10 u10 pbl kindex press temp pm10 Show more#0     1  2    3  4   5   6   7   8      9  10  11  12       13   14   15

#pm25 no2 SO2 nh3 rh  ws  wd  at  prec  v10 u10 pbl kindex  press temp pm10

# Calculating the wind speed
ws=totaldata[:,:,2]

# Calculating the wind direction
wd=totaldata[:,:,3]

# Load the basemap
basemap = gpd.read_file('/content/drive/MyDrive/Modis/Delhi Map/Delhi_Boundary.shp')

# Extracting the gps locations of all the stations.
stations = pd.DataFrame(stn[['Latitude', 'Longitude']])


stations_gdf = gpd.GeoDataFrame(stations, geometry=gpd.points_from_xy(stations['Longitude'], stations['Latitude']), crs=basemap.crs)

stn.head(3)

totaldata.head(3)

basemap.shape
basemap.geometry
basemap.columns
basemap.geometry.centroid.iloc[0]

stations_gdf.shape

# @title Daily data
pm25data=totaldata[:,:,0]   #[dt1:dt2,:,0]

dailydata=[]
for i in range(1095):
  nw=np.mean(pm25data[(8*i):(8*i+8)],axis=0)
  print(nw.shape)
  dailydata.append(nw)
dailydata=np.array(dailydata)

# Create an array of dates from January 1, 2020, 12:00 AM to December 31, 2022, 11:30 PM
date_range = pd.date_range(start="2020-01-01 00:00", end="2022-12-30 23:30", freq="1d")
date_range

# @title Modis data read

modis20=pd.read_csv("/content/drive/MyDrive/Modis/MODIS_AOD/merged_data_2020_sorted_dates.csv")
modis21=pd.read_csv("/content/drive/MyDrive/Modis/MODIS_AOD/merged_data_2021_sorted_dates.csv")
modis22=pd.read_csv("/content/drive/MyDrive/Modis/MODIS_AOD/merged_data_2022_sorted_dates.csv")
modis21_dropped = modis21.drop(columns=['latitude', 'longitude'])
modis22_dropped = modis22.drop(columns=['latitude', 'longitude'])
aod_df=pd.concat([modis20,modis21_dropped,modis22_dropped], axis=1)

modis20.columns

"""# Here we are matching the data sets MODIS `Moderate Resolution Imaging Spectroradiometer` and CPCB `Central Pollution Control Board`"""

# @title MODIS_CPCB matchup

stn_df = pd.DataFrame(stn) # stn = pd.read_csv('/content/drive/MyDrive/Modis/stn_extrafeat.csv', delimiter=',')
aod_df = pd.DataFrame(aod_df) # aod_df=pd.concat([modis20,modis21_dropped,modis22_dropped], axis=1)

# Combine latitude and longitude into a single array for KDTree
stn_coords = stn_df[['Latitude', 'Longitude']].values
aod_coords = aod_df[['latitude', 'longitude']].values

# Create KDTree for aod coordinates
tree = cKDTree(aod_coords)

# Query the nearest neighbor aod_coordintaes for each stn coordinate
distances, indices = tree.query(stn_coords, k=1)

# Get the nearest neighbors' coordinates
nearest_neighbors = aod_df.iloc[indices].reset_index(drop=True)

# # Combine the results with the original stn dataframe
# result = pd.concat([stn_df, nearest_neighbors], axis=1)
# result.columns = ['stn_Latitude', 'stn_Longitude', 'aod_Latitude', 'aod_Longitude']
nearest_neighbors= pd.DataFrame(nearest_neighbors)
nearest_neighbors.to_csv("/content/drive/MyDrive/Modis/MODIS_AOD/MODIS_analysis/CPCB_40_GMS_MODIS_matchup.csv")

# Print the result
print(nearest_neighbors)
null_values = nearest_neighbors.isnull().sum()

# Printing the result
print(null_values)
null_values=null_values[2:]
null_values=40-null_values



null_values.index=pd.to_datetime(null_values.index)

import matplotlib.pyplot as plt

null_values_2020 = null_values[null_values.index.year == 2020]
fig, ax = plt.subplots(figsize=(15, 5))

# Plotting the result
null_values_2020.plot(figsize=(15, 5))
plt.title('Number of available AOD pixel Values in 2020')
plt.xlabel('Date')
plt.ylabel('Number of available Modis AOD match')
plt.xticks(rotation=45)

# Optional: Format the x-axis date labels
ax.xaxis.set_major_formatter(plt.FixedFormatter(null_values_2020.index.strftime("%Y-%m-%d")))

plt.tight_layout()
plt.savefig('/content/drive/MyDrive/Modis/MODIS_AOD/MODIS_analysis/modis2020_gms_cpcb_match_no_of_stations.png')
plt.show()



nearest_neighbors

date_columns

# @title Check for duplicated column names
duplicated_columns = nearest_neighbors.columns[nearest_neighbors.columns.duplicated()].unique()

# Average the duplicated columns
for col in duplicated_columns:
    cols_to_average = nearest_neighbors.loc[:, col]
    averaged_col = cols_to_average.mean(axis=1)
    nearest_neighbors[col] = averaged_col

# Drop the original duplicated columns except for the newly averaged column
nearest_neighbors = nearest_neighbors.loc[:, ~nearest_neighbors.columns.duplicated()]
# Check for duplicated column names
duplicated_columns = nearest_neighbors.columns[nearest_neighbors.columns.duplicated()]

# Print the duplicated column names
print("Duplicated column names:", duplicated_columns)

# @title Station-wise coordinate extraction in format (latitude, longitude, julian day, )
# pm2.5 DATASET
pm_df = pd.DataFrame(data=dailydata, index=date_range, columns=[f'Column{i+1}' for i in range(dailydata.shape[1])])
pm_df=pm_df.T
pm_df

results = []
date_columns = nearest_neighbors.columns[2:]
nearest_neighbors.columns = ['latitude', 'longitude'] + [pd.to_datetime(col) for col in date_columns]

# Loop over the rows in the DataFrame
for index, row in nearest_neighbors.iterrows():
    latitude = row['latitude']
    longitude = row['longitude']
    print("row",row)
    print(index)
    # Loop over the columns (excluding 'latitude' and 'longitude')
    for col in row.index[2:]:
        print("col",col)
        value = row[col]
        # print(value.shape)
        # print(pm_row)
        if pd.notnull(value):
            # Convert column name (date) to Julian day
            date = pd.to_datetime(col)
            julian_day = date.strftime('%j')  # '%j' gives the day of the year
            year = date.year
            month=  date.month
            pm_val= pm_df.iloc[index]
            pm_val=pm_val.loc[(pm_val.index == col)]
            print("pm",pm_val.values)
            # Append the results
            results.append({
                'latitude': latitude,
                 'longitude': longitude,
                 'year': year,
                'month':month,
                'julian_day': int(julian_day),
                'MODIS_AOD': value,
                'PM25' : pm_val.values
            })

# Create a new DataFrame from the results
results_df = pd.DataFrame(results)

# Print the results DataFrame
print(results_df)

# Optional: save the results to a CSV file
results_df.to_csv('/content/drive/MyDrive/VIIRS_SNPP/VIIRS_new/MODIS_AOD/MODIS_analysis/Matched_up_non_null_AOD.csv', index=False)

results_df['PM25']=results_df['PM25'].apply(lambda x: x[0] if isinstance(x, list) else x).astype(float)
results_df.to_csv('/content/drive/MyDrive/VIIRS_SNPP/VIIRS_new/MODIS_AOD/MODIS_analysis/Matched_up_non_null_AOD_pm25.csv', index=False)

mixed_data.to_csv('/content/drive/MyDrive/Spat_estimation/MODIS_AOD/MODIS_analysis/Mixed_MODIS_VIIRS(1km_matchup)_matched_up_non_null_AOD_pm25.csv', index=False)

viirs_df_cleaned.to_csv('/content/drive/MyDrive/Spat_estimation/MODIS_AOD/MODIS_analysis/VIIRS(1km_matchup)_matched_up_non_null_AOD_pm25.csv', index=False)

results_df=pd.read_csv("/content/drive/MyDrive/VIIRS_SNPP/VIIRS_new/MODIS_AOD/MODIS_analysis/Matched_up_non_null_AOD_pm25.csv")

results_df

df = pd.DataFrame(data=dailydata, index=date_range, columns=[f'Column{i+1}' for i in range(dailydata.shape[1])])

modis_data= results_df[['latitude','longitude','julian_day','MODIS_AOD','PM25']]

modis_data.columns=['Latitude','Longitude','Julian_Day','AOD','PM2.5']



viirs_data=pd.read_csv("/content/drive/MyDrive/VIIRS_SNPP/VIIRS_new/VIIRS_PM2.5_Merged(3000m).csv")

stn = pd.read_csv('/content/drive/MyDrive/Pollutants_Data/stn_extrafeat.csv', delimiter=',')

viirs_data.columns

viirs_df=viirs_data[['Datetime','AOD','Station_ID', 'PM2.5']]

viirs_df['Station_Number'] = viirs_df['Station_ID'].str.extract(r'Station_(\d+)')

# Convert the extracted numbers to integers if needed
viirs_df['Station_Number'] = viirs_df['Station_Number'].astype(int)

viirs_df[['Latitude', 'Longitude']] = viirs_df.apply(
    lambda row: stn[['Latitude', 'Longitude']].iloc[row['Station_Number']], axis=1)

stn[['Latitude','Longitude']].iloc[0]

viirs_df_cleaned = viirs_df.dropna(subset=['PM2.5'])

viirs_df_cleaned['Datetime'] = pd.to_datetime(viirs_df_cleaned['Datetime'])
viirs_df_cleaned['Julian_Day'] = viirs_df_cleaned['Datetime'].dt.dayofyear
viirs_df_cleaned.Julian_Day

viirs_df_cleaned

mixed_data=pd.concat([modis_data,viirs_df_cleaned[['Latitude','Longitude','Julian_Day','AOD','PM2.5']]], axis=0)

modis_data

mixed_data





df = pd.read_csv("/content/drive/MyDrive/VIIRS_SNPP/VIIRS_new/MODIS_AOD/MODIS_analysis/VIIRS(1km_matchup)_matched_up_non_null_AOD_pm25.csv")
df.columns

mixed_data.columns

#@title MODIS-VIIRS mixed data
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split, cross_val_score, KFold
import numpy as np
# df = pd.read_csv("/content/drive/MyDrive/VIIRS_SNPP/VIIRS_new/MODIS_AOD/MODIS_analysis/VIIRS(1km_matchup)_matched_up_non_null_AOD_pm25.csv")
# df = df.dropna(subset=['PM2.5'])

# Define input features and target variable
X = mixed_data[['Latitude','Longitude','AOD','Julian_Day']]
y = mixed_data['PM2.5']

# Split the data into training and testing sets
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Random Forest model
rf = RandomForestRegressor(n_estimators=100, random_state=42)

kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Perform cross-validation and calculate metrics for each fold
mse_scores = cross_val_score(rf, X, y, cv=kf, scoring='neg_mean_squared_error')
mae_scores = cross_val_score(rf, X, y, cv=kf, scoring='neg_mean_absolute_error')
r2_scores = cross_val_score(rf, X, y, cv=kf, scoring='r2')

# Calculate the mean and standard deviation for each metric
mean_mse = -mse_scores.mean()
std_mse = mse_scores.std()
mean_rmse = np.sqrt(-mse_scores.mean())
mean_mae = -mae_scores.mean()
std_mae = mae_scores.std()
mean_r2 = r2_scores.mean()
std_r2 = r2_scores.std()

print(f"Mean Squared Error: {round(mean_mse, 2)} ± {round(std_mse, 2)}")
print(f"Root Mean Squared Error: {round(mean_rmse, 2)}")
print(f"Mean Absolute Error: {round(mean_mae, 2)} ± {round(std_mae, 2)}")
print(f"R^2 Score: {round(mean_r2, 2)} ± {round(std_r2, 2)}")

# Perform cross-validation to calculate MAPE and Bias
mape_scores = []
bias_scores = []

for train_index, test_index in kf.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # Train the model
    rf.fit(X_train, y_train)

    # Make predictions
    y_pred = rf.predict(X_test)

    # Calculate MAPE
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    mape_scores.append(mape)

    # Calculate Bias
    bias = np.mean(y_pred - y_test)
    bias_scores.append(bias)

mean_mape = round(np.mean(mape_scores),2)
std_mape = round(np.std(mape_scores),2)
mean_bias = round(np.mean(bias_scores),2)
std_bias = round(np.std(bias_scores),2)

print(f"Mean Absolute Percentage Error: {mean_mape} ± {std_mape}")
print(f"Bias: {mean_bias} ± {std_bias}")

import pandas as pd
import torch
from torch import nn
from torch.utils.data import DataLoader, TensorDataset, random_split
import numpy as np
from sklearn.preprocessing import StandardScaler

!pip install pyro-ppl==1.8.4

import pandas as pd
import torch
from torch import nn
from torch.utils.data import DataLoader, TensorDataset, random_split
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import pyro
import pyro.distributions as dist
from pyro.nn import PyroSample, PyroModule
from pyro.infer import SVI, Trace_ELBO
from pyro.optim import Adam

#@title Bayesian Neural Network Implementation
df = pd.read_csv("/content/drive/MyDrive/VIIRS_SNPP/VIIRS_new/VIIRS_PM2.5_Merged(500m).csv")
X = df[['Station_Latitude','Station_Longitude','AOD']].values
y = df['PM2.5'].values

# Normalize the data
scaler_X = StandardScaler()
scaler_y = StandardScaler()
X = scaler_X.fit_transform(X)
y = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()

# Convert to PyTorch tensors
X_tensor = torch.tensor(X, dtype=torch.float32)
y_tensor = torch.tensor(y, dtype=torch.float32)

# Create dataset and dataloaders
dataset = TensorDataset(X_tensor, y_tensor)
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = random_split(dataset, [train_size, test_size])
train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)

# Define the Bayesian Neural Network model
class BayesianNN(PyroModule):
    def __init__(self, input_dim, hidden_dim):
        super(BayesianNN, self).__init__()
        self.fc1 = PyroModule[nn.Linear](input_dim, hidden_dim)
        self.fc1.weight = PyroSample(dist.Normal(0., 1.).expand([hidden_dim, input_dim]).to_event(2))
        self.fc1.bias = PyroSample(dist.Normal(0., 1.).expand([hidden_dim]).to_event(1))

        self.fc2 = PyroModule[nn.Linear](hidden_dim, hidden_dim)
        self.fc2.weight = PyroSample(dist.Normal(0., 1.).expand([hidden_dim, hidden_dim]).to_event(2))
        self.fc2.bias = PyroSample(dist.Normal(0., 1.).expand([hidden_dim]).to_event(1))

        self.out = PyroModule[nn.Linear](hidden_dim, 1)
        self.out.weight = PyroSample(dist.Normal(0., 1.).expand([1, hidden_dim]).to_event(2))
        self.out.bias = PyroSample(dist.Normal(0., 1.).expand([1]).to_event(1))

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.out(x)

bnn = BayesianNN(input_dim=X.shape[1], hidden_dim=10)

# Bayesian model and guide functions
def model(x_data, y_data):
    pyro.module("bnn", bnn)
    with pyro.plate("data", x_data.shape[0]):
        prediction_mean = bnn(x_data).squeeze(-1)
        pyro.sample("obs", dist.Normal(prediction_mean, 1.), obs=y_data)

def guide(x_data, y_data):
    pyro.module("bnn", bnn)
    # Use the variational distribution defined in PyroModule

# SVI (Stochastic Variational Inference)
optim = Adam({"lr": 0.01})
svi = SVI(model, guide, optim, loss=Trace_ELBO())

# Training the model
num_iterations = 5000
for epoch in range(num_iterations):
    loss = 0
    for batch_X, batch_y in train_loader:
        loss += svi.step(batch_X, batch_y)
    if epoch % 500 == 0:
        print(f"Epoch {epoch} - Loss: {loss / len(train_loader.dataset)}")

# Predicting on test set
def predict(x_data):
    sampled_models = [guide(None, None) for _ in range(100)]
    yhats = [model(x_data).data for model in sampled_models]
    mean = torch.mean(torch.stack(yhats), 0)
    return mean

bnn.eval()
with torch.no_grad():
    predictions = []
    actuals = []
    for batch_X, batch_y in test_loader:
        outputs = predict(batch_X)
        predictions.extend(outputs.numpy())
        actuals.extend(batch_y.numpy())

# Reverse the normalization
predictions = scaler_y.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()
actuals = scaler_y.inverse_transform(np.array(actuals).reshape(-1, 1)).flatten()

# Calculate evaluation metrics
r2 = r2_score(actuals, predictions)
rmse = np.sqrt(mean_squared_error(actuals, predictions))
mae = mean_absolute_error(actuals, predictions)
mape = np.mean(np.abs((np.array(actuals) - np.array(predictions)) / np.array(actuals))) * 100
bias = np.mean(np.array(predictions) - np.array(actuals))

# Print evaluation metrics
print(f"R2: {r2}")
print(f"RMSE: {rmse}")
print(f"MAE: {mae}")
print(f"MAPE: {mape}")
print(f"Bias: {bias}")

X = df[['Station_Latitude','Station_Longitude','AOD']].values
y = df['PM2.5'].values
scaler_X = StandardScaler()
scaler_y = StandardScaler()
X = scaler_X.fit_transform(X)
y = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()

# Convert to PyTorch tensors
X_tensor = torch.tensor(X, dtype=torch.float32)
y_tensor = torch.tensor(y, dtype=torch.float32)

# Create dataset and dataloaders
dataset = TensorDataset(X_tensor, y_tensor)
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = random_split(dataset, [train_size, test_size])
train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)

# Define the MLP model
class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.out = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.out(x)

mlp = MLP(input_dim=X.shape[1], hidden_dim=10)

# Loss function and optimizer
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)

# Train the model
num_epochs = 100
for epoch in range(num_epochs):
    mlp.train()
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = mlp(batch_X)
        loss = criterion(outputs.squeeze(), batch_y)
        loss.backward()
        optimizer.step()
    if epoch % 10 == 0:
        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}")

# Evaluate the model
mlp.eval()
with torch.no_grad():
    predictions = []
    actuals = []
    for batch_X, batch_y in test_loader:
        outputs = mlp(batch_X)
        predictions.extend(outputs.numpy())
        actuals.extend(batch_y.numpy())

# Reverse the normalization
predictions = scaler_y.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()
actuals = scaler_y.inverse_transform(np.array(actuals).reshape(-1, 1)).flatten()

# Calculate evaluation metrics
r2 = r2_score(actuals, predictions)
rmse = np.sqrt(mean_squared_error(actuals, predictions))
mae = mean_absolute_error(actuals, predictions)
mape = np.mean(np.abs((np.array(actuals) - np.array(predictions)) / np.array(actuals))) * 100
bias = np.mean(np.array(predictions) - np.array(actuals))

# Print evaluation metrics
print(f"R2: {r2}")
print(f"RMSE: {rmse}")
print(f"MAE: {mae}")
print(f"MAPE: {mape}")
print(f"Bias: {bias}")

X = df[['Station_Latitude','Station_Longitude','AOD']].values
y = df['PM2.5'].values

# Normalize the data
scaler_X = StandardScaler()
scaler_y = StandardScaler()
X = scaler_X.fit_transform(X)
y = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()

# Convert to PyTorch tensors
X_tensor = torch.tensor(X, dtype=torch.float32)
y_tensor = torch.tensor(y, dtype=torch.float32)

# Create dataset and dataloaders
dataset = TensorDataset(X_tensor, y_tensor)
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = random_split(dataset, [train_size, test_size])
train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)

# Define the Transformer model
class TransformerRegressor(nn.Module):
    def __init__(self, input_dim, output_dim, num_heads, num_layers, dim_feedforward, dropout=0.1):
        super(TransformerRegressor, self).__init__()
        self.input_dim = input_dim
        self.transformer = nn.Transformer(
            d_model=input_dim, nhead=num_heads, num_encoder_layers=num_layers,
            dim_feedforward=dim_feedforward, dropout=dropout
        )
        self.fc = nn.Linear(input_dim, output_dim)

    def forward(self, x):
        x = x.unsqueeze(1)  # (batch_size, 1, input_dim)
        x = self.transformer(x, x)  # (batch_size, 1, input_dim)
        x = x.squeeze(1)  # (batch_size, input_dim)
        x = self.fc(x)  # (batch_size, output_dim)
        return x

# Instantiate the model, loss function, and optimizer
input_dim = X.shape[1]
output_dim = 1
model = TransformerRegressor(input_dim, output_dim, num_heads=2, num_layers=2, dim_feedforward=64)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Train the model
num_epochs = 100
for epoch in range(num_epochs):
    model.train()
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}")

# Evaluate the model
model.eval()
with torch.no_grad():
    predictions = []
    actuals = []
    for batch_X, batch_y in test_loader:
        outputs = model(batch_X)
        predictions.extend(outputs.numpy())
        actuals.extend(batch_y.numpy())

# Reverse the normalization
predictions = scaler_y.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()
actuals = scaler_y.inverse_transform(np.array(actuals).reshape(-1, 1)).flatten()

# Print predictions and actual values
print("Predictions:", predictions)
print("Actuals:", actuals)

# Calculate evaluation metrics
r2 = r2_score(actuals, predictions)
rmse = np.sqrt(mean_squared_error(actuals, predictions))
mae = mean_absolute_error(actuals, predictions)
mape = np.mean(np.abs((np.array(actuals) - np.array(predictions)) / np.array(actuals))) * 100
bias = np.mean(np.array(predictions) - np.array(actuals))

# Print evaluation metrics
print(f"R2: {r2}")
print(f"RMSE: {rmse}")
print(f"MAE: {mae}")
print(f"MAPE: {mape}")
print(f"Bias: {bias}")

df = pd.DataFrame(viirs_df_cleaned)
df.columns

import pandas as pd
df = pd.read_csv('/content/drive/MyDrive/VIIRS_SNPP/Delhi_AOD_2022_pivoted.csv')
df.columns

import pandas as pd


df = pd.read_csv('/content/drive/MyDrive/VIIRS_SNPP/Delhi_AOD_2022_pivoted.csv')

# Melt the DataFrame into long format
df_long = pd.melt(df, id_vars=['Latitude', 'Longitude'], var_name='Date', value_name='MODIS_AOD')

# Ensure 'Date' column is of datetime type and sort by 'Date'
df_long['Date'] = pd.to_datetime(df_long['Date'])
df_long = df_long.sort_values(by='Date')
df_long['julian_day'] = df_long['Date'].dt.dayofyear
# Display the resulting DataFrame
print(df_long)

import pandas as pd
df = pd.read_csv('/content/drive/MyDrive/VIIRS_SNPP/Delhi_AOD_2022_pivoted.csv')
df.columns

import pandas as pd
df = pd.read_csv('/content/drive/MyDrive/VIIRS_SNPP/VIIRS_new/MODIS_AOD/MODIS_analysis/Matched_up_non_null_AOD_pm25.csv')
df.columns

import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# Load the dataset (assuming it's already loaded in df)
df = pd.read_csv('/content/drive/MyDrive/VIIRS_SNPP/VIIRS_new/MODIS_AOD/MODIS_analysis/Matched_up_non_null_AOD_pm25.csv')  # Update with your CSV path

# Define input features and target variable
X = df[['latitude', 'longitude', 'MODIS_AOD', 'julian_day']]
y = df['PM25']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Random Forest model
rf = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model
rf.fit(X_train, y_train)

# Save the model using joblib
joblib.dump(rf, 'random_forest_model.pkl')

# Make predictions
y_pred = rf.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
mape = (abs((y_test - y_pred) / y_test).mean()) * 100
rmse = mse ** 0.5
mae = mean_absolute_error(y_test, y_pred)

# Calculate Bias
bias = (y_pred - y_test).mean()

print(f"Mean Squared Error: {mse}")
print(f"Root Mean Squared Error: {rmse}")
print(f"R^2 Score: {r2}")
print(f"Mean Absolute Percentage Error: {mape}")
print(f"Mean Absolute Error: {mae}")
print(f"Bias: {bias}")

df_long
df_long = df_long.rename(columns={
    'Longitude': 'longitude',
    'Latitude': 'latitude',
})

import pandas as pd
import joblib

# Load the pre-trained model
rf_model = joblib.load('random_forest_model.pkl')

def predict_pm25(input_data):
    # Check if the input is a DataFrame
    if isinstance(input_data, pd.DataFrame):
        new_data = input_data
    else:
        # Load new data from the CSV file if the input is a path
        new_data = pd.read_csv(input_data)

    # Filter rows where MODIS_AOD is not null
    new_data = new_data[new_data['MODIS_AOD'].notna()]

    # Define the features for prediction
    X_new = new_data[['latitude', 'longitude', 'MODIS_AOD', 'julian_day']]

    # Predict PM2.5 values
    predictions = rf_model.predict(X_new)

    # Add the predictions to the DataFrame
    new_data['Predicted_PM25'] = predictions
    return new_data[['latitude', 'longitude', 'MODIS_AOD', 'julian_day', 'Predicted_PM25']]

# Assuming 'df_long' is your DataFrame and contains the relevant data
df_long['Date'] = pd.to_datetime(df_long['Date'])

# Define the list of dates you want to predict for (specific October dates)
predict_dates = ['2022-10-17', '2022-10-19', '2022-10-21', '2022-10-22', '2022-10-23', '2022-10-24', '2022-10-26', '2022-10-28', '2022-10-29']

# Filter rows where 'Date' is in the 'predict_dates' list
df_predict = df_long[df_long['Date'].isin(pd.to_datetime(predict_dates))]

# Call the prediction function with the filtered DataFrame
predicted_data = predict_pm25(df_predict)

# Print the predicted PM2.5 data for the selected dates
print(predicted_data)
print(predicted_data.head())

len(mixed_data)

predicted_data.to_csv('/content/drive/MyDrive/VIIRS_SNPP/VIIRS_new/MODIS_VIIRS_OCT_predicted_data.csv')





from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score

# Define input features and target variable
X = df[['latitude', 'longitude','MODIS_AOD','julian_day']]
y = df['PM25']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Random Forest model
svr = SVR(kernel='rbf', C=100, gamma='auto')

# Train the model
svr.fit(X_train, y_train)

# Make predictions
y_pred = svr.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
# Calculate MAPE
mape = (abs((y_test - y_pred) / y_test).mean()) * 100
rmse = mse ** 0.5
mae = mean_absolute_error(y_test, y_pred)

# Calculate Bias
bias = (y_pred - y_test).mean()

print(f"Mean Squared Error: {mse}")
print(f"Root Mean Squared Error: {rmse}")
print(f"R^2 Score: {r2}")
print(f"Mean Absolute Percentage Error: {mape}")
print(f"Mean Absolute Error: {mae}")

print(f"Bias: {bias}")

# Print predictions
# print("Predictions:", y_pred)
print(f"Mean Squared Error: {mse}")

# Function to determine the season based on month and day
def get_season(month):
    if month in [12, 1, 2, 3]:  # Winter
        return 'Winter'
    elif month in [4, 5, 6]:  # Summer/Pre-monsoon
        return 'Summer'
    elif month in [7, 8, 9]:  # Monsoon/Rainy
        return 'Monsoon'
    elif month in [10, 11]:  # Post-monsoon
        return 'Post-monsoon'
    else:
        return 'Unknown'

# Add a 'season' column to the DataFrame
results_df['season'] = results_df.apply(lambda row: get_season(row['month']), axis=1)

# Print the updated DataFrame
print(results_df)

winter_samples = results_df[results_df['season'] == 'Winter']
summer_samples = results_df[results_df['season'] == 'Summer']
monsoon_samples = results_df[results_df['season'] == 'Monsoon']
postmonsoon_samples = results_df[results_df['season'] == 'Post-monsoon']

season_df= winter_samples

print(results_df.describe())

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split, cross_val_score, KFold

df = pd.DataFrame(results_df)

# Define input features and target variable
X = df[['latitude', 'longitude', 'julian_day']]
y = df['PM25']

# Split the data into training and testing sets
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Random Forest model
rf = RandomForestRegressor(n_estimators=100, random_state=42)

kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Perform cross-validation and calculate metrics for each fold
mse_scores = cross_val_score(rf, X, y, cv=kf, scoring='neg_mean_squared_error')
mae_scores = cross_val_score(rf, X, y, cv=kf, scoring='neg_mean_absolute_error')
r2_scores = cross_val_score(rf, X, y, cv=kf, scoring='r2')

# Calculate the mean and standard deviation for each metric
mean_mse = -mse_scores.mean()
std_mse = mse_scores.std()
mean_rmse = np.sqrt(-mse_scores.mean())
mean_mae = -mae_scores.mean()
std_mae = mae_scores.std()
mean_r2 = r2_scores.mean()
std_r2 = r2_scores.std()

print(f"Mean Squared Error: {mean_mse} ± {std_mse}")
print(f"Root Mean Squared Error: {mean_rmse}")
print(f"Mean Absolute Error: {mean_mae} ± {std_mae}")
print(f"R^2 Score: {mean_r2} ± {std_r2}")

# Perform cross-validation to calculate MAPE and Bias
mape_scores = []
bias_scores = []

for train_index, test_index in kf.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # Train the model
    rf.fit(X_train, y_train)

    # Make predictions
    y_pred = rf.predict(X_test)

    # Calculate MAPE
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    mape_scores.append(mape)

    # Calculate Bias
    bias = np.mean(y_pred - y_test)
    bias_scores.append(bias)

mean_mape = np.mean(mape_scores)
std_mape = np.std(mape_scores)
mean_bias = np.mean(bias_scores)
std_bias = np.std(bias_scores)

print(f"Mean Absolute Percentage Error: {mean_mape} ± {std_mape}")
print(f"Bias: {mean_bias} ± {std_bias}")

plot_df =pd.read_csv('/content/drive/MyDrive/VIIRS_SNPP/VIIRS_new/MODIS_OCT_predicted_data.csv')
plot_df.columns

import geopandas as gpd
import matplotlib.pyplot as plt
import numpy as np
from scipy.interpolate import griddata
from shapely.geometry import Point

# Load the shapefile
delhi_shapefile = "/content/drive/MyDrive/Delhi Map/Delhi_Boundary.shp"
delhi_map = gpd.read_file(delhi_shapefile)
delhi_map = delhi_map.to_crs(epsg=4326)

# Define the dates for which plots are needed
dates_to_plot = [290]

# Find the global min and max values for PM2.5 across all Julian days
global_min = np.inf
global_max = -np.inf

for julian_day in dates_to_plot:
    df_julian = predicted_data[predicted_data['julian_day'] == julian_day]
    pm25_values = df_julian['Predicted_PM25'].values
    global_min = min(global_min, pm25_values.min())
    global_max = max(global_max, pm25_values.max())

# Loop through each date and generate the plot
for julian_day in dates_to_plot:
    # Filter the data for the current Julian day
    df_julian = predicted_data[predicted_data['julian_day'] == julian_day]
    longitude = df_julian['longitude'].values
    latitude = df_julian['latitude'].values
    predicted_pm25 = df_julian['Predicted_PM25'].values

    # Create a grid for interpolation
    grid_x, grid_y = np.mgrid[
        longitude.min():longitude.max():500j,
        latitude.min():latitude.max():500j
    ]

    # Interpolate the scattered data to the grid
    grid_pm25 = griddata(
        (longitude, latitude), predicted_pm25, (grid_x, grid_y), method='cubic'
    )

    # Create grid points and check which are inside the Delhi boundary
    grid_points = np.array([grid_x.flatten(), grid_y.flatten()]).T
    grid_geo = gpd.GeoSeries([Point(xy) for xy in grid_points], crs='EPSG:4326')
    in_boundary = np.array(
        [delhi_map.contains(point).any() for point in grid_geo]
    ).reshape(grid_x.shape)

    # Mask the grid data to include only points inside the boundary
    grid_pm25_masked = np.ma.masked_where(~in_boundary, grid_pm25)

    # Plot the heatmap
    plt.figure(figsize=(10, 6))
    delhi_map.plot(ax=plt.gca(), color='none', edgecolor='black', alpha=0.5)
    plt.imshow(
        grid_pm25_masked.T,
        extent=(longitude.min(), longitude.max(), latitude.min(), latitude.max()),
        origin='lower',
        cmap='viridis',
        alpha=0.7,
        vmin= 50,  # Set global minimum for color scale
        vmax=300,  # Set global maximum for color scale
    )
    plt.colorbar(label='Predicted PM2.5')
    plt.title(f'Predicted $PM_{{2.5}}$ Heatmap for julian_day {julian_day} in Delhi')
    plt.xlabel('Longitude')
    plt.ylabel('Latitude')

    # Save the plot as a TIFF file
    plt.savefig(f'PM2.5_MODIS_VIIRS_Pred_{julian_day}.tiff', format='tiff')
    plt.close()

print("Plots have been saved successfully!")

import geopandas as gpd
import matplotlib.pyplot as plt
from shapely.geometry import Point

# Load Delhi shapefile and set CRS
delhi_shapefile = "/content/drive/MyDrive/Delhi Map/Delhi_Boundary.shp"
delhi_map = gpd.read_file(delhi_shapefile)
delhi_map = delhi_map.to_crs(epsg=4326)

# Filter data for julian_day 290
df_julian_290 = predicted_data[predicted_data['julian_day'] == 290]
longitude = df_julian_290['longitude'].values
latitude = df_julian_290['latitude'].values
predicted_pm25 = df_julian_290['Predicted_PM25'].values

# Create a GeoDataFrame for the PM2.5 points
geometry = [Point(xy) for xy in zip(longitude, latitude)]
pm25_gdf = gpd.GeoDataFrame(df_julian_290, geometry=geometry, crs='EPSG:4326')

# Filter points inside the Delhi boundary
pm25_gdf_in_boundary = gpd.sjoin(pm25_gdf, delhi_map, how='inner', predicate='intersects')

# Plot the map
plt.figure(figsize=(10, 6))
ax = plt.gca()

# Plot Delhi boundary
delhi_map.plot(ax=ax, color='none', edgecolor='black', linewidth=1.5, alpha=0.7)

# Scatter plot for PM2.5 points within the boundary with square markers
scatter = plt.scatter(
    pm25_gdf_in_boundary.geometry.x,
    pm25_gdf_in_boundary.geometry.y,
    c=pm25_gdf_in_boundary['Predicted_PM25'],
    cmap='viridis',
    s=1000,  # Adjust marker size
    alpha=0.7,
    marker='s'  # Use square markers
)

# Add color bar and labels
plt.colorbar(scatter, label='Predicted PM2.5')
plt.title('Predicted $PM_{2.5}$ Heatmap for julian_day 290 in Delhi')
plt.xlabel('Longitude')
plt.ylabel('Latitude')

# Set map extent for Delhi
ax.set_xlim([76.8, 77.4])
ax.set_ylim([28.4, 28.9])

plt.show()

import numpy as np
from scipy.spatial import cKDTree
import geopandas as gpd
import matplotlib.pyplot as plt
from shapely.geometry import Point

# Load Delhi shapefile and set CRS
delhi_shapefile = "/content/drive/MyDrive/Delhi Map/Delhi_Boundary.shp"
delhi_map = gpd.read_file(delhi_shapefile)
delhi_map = delhi_map.to_crs(epsg=4326)

# Filter data for julian_day 290
df_julian_290 = predicted_data[predicted_data['julian_day'] == 290]
longitude = df_julian_290['longitude'].values
latitude = df_julian_290['latitude'].values
predicted_pm25 = df_julian_290['Predicted_PM25'].values

# Create a GeoDataFrame for the PM2.5 points
geometry = [Point(xy) for xy in zip(longitude, latitude)]
pm25_gdf = gpd.GeoDataFrame(df_julian_290, geometry=geometry, crs='EPSG:4326')

# Filter points inside the Delhi boundary
pm25_gdf_in_boundary = gpd.sjoin(pm25_gdf, delhi_map, how='inner', predicate='intersects')

# Grid creation (cover the Delhi region)
x_min, x_max = delhi_map.total_bounds[0], delhi_map.total_bounds[2]
y_min, y_max = delhi_map.total_bounds[1], delhi_map.total_bounds[3]
grid_x, grid_y = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))

# Flatten the grid for interpolation
grid_points = np.vstack((grid_x.flatten(), grid_y.flatten())).T

# IDW Interpolation function
def idw_interpolation(grid_points, data_points, values, power=2):
    tree = cKDTree(data_points)  # KDTree for fast nearest neighbor search
    distances, indices = tree.query(grid_points, k=5)  # Find 5 nearest points

    # Apply IDW formula
    weights = 1 / (distances ** power)  # Inverse distance weighting (with power)
    weighted_values = weights / weights.sum(axis=1)[:, None] * values[indices]
    return weighted_values.sum(axis=1)

# Perform IDW interpolation for PM2.5 values on the grid
interpolated_pm25 = idw_interpolation(grid_points, np.column_stack((longitude, latitude)), predicted_pm25)

# Reshape the interpolated values to match the grid shape
interpolated_pm25_grid = interpolated_pm25.reshape(grid_x.shape)

# Mask the grid to keep only values inside Delhi's boundary
mask = delhi_map.geometry.union_all()  # Use 'union_all()' to avoid deprecation warning
mask_polygon = mask

# Create a mask for grid points inside Delhi boundary
mask_grid = np.array([mask_polygon.contains(Point(x, y)) for x, y in grid_points])

# Reshape mask_grid to match the grid shape
mask_grid_reshaped = mask_grid.reshape(grid_x.shape)

# Apply the mask to the interpolated grid
interpolated_pm25_grid_masked = np.ma.masked_where(~mask_grid_reshaped, interpolated_pm25_grid)

# Plot the results
plt.figure(figsize=(10, 6))
ax = plt.gca()

# Plot Delhi boundary
delhi_map.plot(ax=ax, color='none', edgecolor='black', linewidth=1.5, alpha=0.7)

# Plot the masked interpolated PM2.5 grid as a heatmap
c = ax.pcolormesh(grid_x, grid_y, interpolated_pm25_grid_masked, cmap='viridis', shading='auto')

# Add color bar
plt.colorbar(c, label='Interpolated PM2.5')

# Add scatter plot for actual data points
scatter = plt.scatter(
    pm25_gdf_in_boundary.geometry.x,
    pm25_gdf_in_boundary.geometry.y,
    c=pm25_gdf_in_boundary['Predicted_PM25'],
    cmap='viridis',
    s=100,  # Adjust marker size
    alpha=0.7,
    marker='s'  # Use square markers
)

# Title and labels
plt.title('Predicted $PM_{2.5}$ Heatmap with IDW Interpolation for julian_day 290 in Delhi')
plt.xlabel('Longitude')
plt.ylabel('Latitude')

# Set map extent for Delhi
ax.set_xlim([x_min, x_max])
ax.set_ylim([y_min, y_max])

plt.show()

df_long['Date'] = pd.to_datetime(df_long['Date'])

# Filter the rows for the specific date
filtered_df = df_long[df_long['Date'] == '2022-10-17']

# Count how many samples are available for this date
num_samples = len(filtered_df)

# Output the number of samples
print(f'Number of PM2.5 samples for 2022-10-17: {num_samples}')

!pip install rasterio

import numpy as np
from scipy.spatial import cKDTree
import geopandas as gpd
import matplotlib.pyplot as plt
from shapely.geometry import Point
from matplotlib import cm
from rasterio import Affine, MemoryFile
from rasterio.enums import Resampling

# Load Delhi shapefile and set CRS
delhi_shapefile = "/content/drive/MyDrive/Delhi Map/Delhi_Boundary.shp"
delhi_map = gpd.read_file(delhi_shapefile)
delhi_map = delhi_map.to_crs(epsg=4326)

# Julian days to process
julian_days = [290, 292, 294, 295, 296, 297, 299, 301, 302]

# Create a grid (common for all days)
x_min, x_max = delhi_map.total_bounds[0], delhi_map.total_bounds[2]
y_min, y_max = delhi_map.total_bounds[1], delhi_map.total_bounds[3]
grid_x, grid_y = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))

# Flatten the grid for interpolation
grid_points = np.vstack((grid_x.flatten(), grid_y.flatten())).T

# IDW Interpolation function
def idw_interpolation(grid_points, data_points, values, power=2):
    tree = cKDTree(data_points)  # KDTree for fast nearest neighbor search
    distances, indices = tree.query(grid_points, k=5)  # Find 5 nearest points

    # Apply IDW formula
    weights = 1 / (distances ** power)  # Inverse distance weighting (with power)
    weighted_values = weights / weights.sum(axis=1)[:, None] * values[indices]
    return weighted_values.sum(axis=1)

# Step 1: Calculate global min and max for consistent color range
global_min = float('inf')
global_max = float('-inf')

for julian_day in julian_days:
    df_julian = predicted_data[predicted_data['julian_day'] == julian_day]
    predicted_pm25 = df_julian['Predicted_PM25'].values
    global_min = min(global_min, predicted_pm25.min())
    global_max = max(global_max, predicted_pm25.max())

# Step 2: Loop over each julian_day to create and save the heatmap
for julian_day in julian_days:
    # Filter data for the specific julian_day
    df_julian = predicted_data[predicted_data['julian_day'] == julian_day]
    longitude = df_julian['longitude'].values
    latitude = df_julian['latitude'].values
    predicted_pm25 = df_julian['Predicted_PM25'].values

    # Create a GeoDataFrame for the PM2.5 points
    geometry = [Point(xy) for xy in zip(longitude, latitude)]
    pm25_gdf = gpd.GeoDataFrame(df_julian, geometry=geometry, crs='EPSG:4326')

    # Filter points inside the Delhi boundary
    pm25_gdf_in_boundary = gpd.sjoin(pm25_gdf, delhi_map, how='inner', predicate='intersects')

    # Perform IDW interpolation for PM2.5 values on the grid
    interpolated_pm25 = idw_interpolation(grid_points, np.column_stack((longitude, latitude)), predicted_pm25)

    # Reshape the interpolated values to match the grid shape
    interpolated_pm25_grid = interpolated_pm25.reshape(grid_x.shape)

    # Mask the grid to keep only values inside Delhi's boundary
    mask = delhi_map.geometry.union_all()  # Use 'union_all()' to avoid deprecation warning
    mask_polygon = mask

    # Create a mask for grid points inside Delhi boundary
    mask_grid = np.array([mask_polygon.contains(Point(x, y)) for x, y in grid_points])

    # Reshape mask_grid to match the grid shape
    mask_grid_reshaped = mask_grid.reshape(grid_x.shape)

    # Apply the mask to the interpolated grid
    interpolated_pm25_grid_masked = np.ma.masked_where(~mask_grid_reshaped, interpolated_pm25_grid)

    # Plot the results for each julian_day
    fig, ax = plt.subplots(figsize=(10, 6))

    # Plot Delhi boundary
    delhi_map.plot(ax=ax, color='none', edgecolor='black', linewidth=1.5, alpha=0.7)

    # Plot the masked interpolated PM2.5 grid as a heatmap with consistent color range
    c = ax.pcolormesh(
        grid_x, grid_y, interpolated_pm25_grid_masked,
        cmap='viridis', shading='auto', vmin=global_min, vmax=global_max
    )

    # Add color bar
    plt.colorbar(c, label='Interpolated PM2.5')

    # Add scatter plot for actual data points
    scatter = plt.scatter(
        pm25_gdf_in_boundary.geometry.x,
        pm25_gdf_in_boundary.geometry.y,
        c=pm25_gdf_in_boundary['Predicted_PM25'],
        cmap='viridis',
        s=100,  # Adjust marker size
        alpha=0.7,
        marker='s',  # Use square markers
        vmin=global_min, vmax=global_max
    )

    # Title and labels
    plt.title(f'Predicted $PM_{2.5}$ Heatmap with IDW Interpolation for julian_day {julian_day} in Delhi')
    plt.xlabel('Longitude')
    plt.ylabel('Latitude')

    # Set map extent for Delhi
    ax.set_xlim([x_min, x_max])
    ax.set_ylim([y_min, y_max])

    # Save the figure as a TIFF file for each day
    output_filename = f"/content/Final_{julian_day}_modis_viirs.tiff"
    plt.savefig(output_filename, format='tiff', dpi=300, bbox_inches='tight')
    plt.close(fig)

    print(f"Heatmap for julian_day {julian_day} saved as {output_filename}")