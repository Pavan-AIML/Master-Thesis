{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to .venvn (3.9.22) (Python 3.9.22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1622f563-3078-485a-a56d-b4677a14135a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'configs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m/Users/pavankumar/Documents/Winter_Thesis/Coding_Learning/Master-Thesis/src/training/trainer_latlon_AOD_PM25.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mconfigs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m config\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'configs'"
     ]
    }
   ],
   "source": [
    "from configs.utils import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd65150-c1cd-48a1-9836-f986e65d1065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pavankumar/Documents/Winter_Thesis/Coding_Learning/Master-Thesis\n",
      "/Users/pavankumar/Documents/Winter_Thesis/Coding_Learning\n",
      "{'dataset_num_1': 1, 'dataset_num_2': 2, 'dataset_num_3': 3, 'exp_name': 'aod_pm25_prediction', 'input_type_1': 1, 'input_type_2': 2, 'input_type_3': 3, 'input_type_4': 4, 'target_type': ['AOD', 'PM2.5'], 'input_vars': {1: ['latitude', 'longitude', 'AOD', 'PM2.5'], 2: ['latitude', 'longitude', 'AOD'], 3: ['latitude', 'longitude', 'PM2.5'], 4: ['latitude', 'longitude']}}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Importing the necessary packages.............\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import arrow\n",
    "from datetime import datetime\n",
    "\n",
    "# from utils.config import config\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "ROOT = Path(__file__).resolve().parents[2]  # tests/ -> project root\n",
    "sys.path.insert(0, str(ROOT))\n",
    "print(ROOT)\n",
    "\n",
    "from configs.utils import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3efdf34-614b-4045-85a5-833f2e5f88f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pavankumar/Documents/Winter_Thesis/Coding_Learning/Master-Thesis\n",
      "/Users/pavankumar/Documents/Winter_Thesis/Coding_Learning/Master-Thesis\n",
      "/Users/pavankumar/Documents/Winter_Thesis/Coding_Learning/Master-Thesis\n",
      "/Users/pavankumar/Documents/Winter_Thesis/Coding_Learning/Master-Thesis\n",
      "Extracting daily Pm2.5 data..\n",
      " daily_PM_data shape: (40, 1095)\n",
      "Fusing AOD data from multiple years...\n",
      " AOD_final shape: (1350, 1098)\n",
      "Finding nearest neighbours for PM2.5 stations...\n",
      "nearest_neighbours shape: (40, 1098)\n",
      "Cleaning data and handling missing values...\n",
      "Cleaned nearest_neighbours shape: (40, 1097)\n",
      "Preparing final training data...\n",
      "final_training_data shape: (15667, 5)\n",
      "Extracting daily Pm2.5 data..\n",
      " daily_PM_data shape: (40, 1095)\n",
      "Fusing AOD data from multiple years...\n",
      " AOD_final shape: (1350, 1098)\n",
      "Finding nearest neighbours for PM2.5 stations...\n",
      "nearest_neighbours shape: (40, 1098)\n",
      "Cleaning data and handling missing values...\n",
      "Cleaned nearest_neighbours shape: (40, 1097)\n",
      "Preparing final training data...\n",
      "final_training_data shape: (15667, 5)\n",
      "Extracting daily Pm2.5 data..\n",
      " daily_PM_data shape: (40, 1095)\n",
      "Fusing AOD data from multiple years...\n",
      " AOD_final shape: (1350, 1098)\n",
      "Finding nearest neighbours for PM2.5 stations...\n",
      "nearest_neighbours shape: (40, 1098)\n",
      "Cleaning data and handling missing values...\n",
      "Cleaned nearest_neighbours shape: (40, 1097)\n",
      "Preparing final training data...\n",
      "final_training_data shape: (15667, 5)\n",
      "Extracting daily Pm2.5 data..\n",
      " daily_PM_data shape: (40, 1095)\n",
      "Fusing AOD data from multiple years...\n",
      " AOD_final shape: (1350, 1098)\n",
      "Finding nearest neighbours for PM2.5 stations...\n",
      "nearest_neighbours shape: (40, 1098)\n",
      "Cleaning data and handling missing values...\n",
      "Cleaned nearest_neighbours shape: (40, 1097)\n",
      "Preparing final training data...\n",
      "final_training_data shape: (15667, 5)\n",
      "0 torch.Size([12307, 128])\n",
      "1 torch.Size([12307, 2])\n",
      "Extracting daily Pm2.5 data..\n",
      " daily_PM_data shape: (40, 1095)\n",
      "Fusing AOD data from multiple years...\n",
      " AOD_final shape: (1350, 1098)\n",
      "Finding nearest neighbours for PM2.5 stations...\n",
      "nearest_neighbours shape: (40, 1098)\n",
      "Cleaning data and handling missing values...\n",
      "Cleaned nearest_neighbours shape: (40, 1097)\n",
      "Preparing final training data...\n",
      "final_training_data shape: (15667, 5)\n",
      "Extracting daily Pm2.5 data..\n",
      " daily_PM_data shape: (40, 1095)\n",
      "Fusing AOD data from multiple years...\n",
      " AOD_final shape: (1350, 1098)\n",
      "Finding nearest neighbours for PM2.5 stations...\n",
      "nearest_neighbours shape: (40, 1098)\n",
      "Cleaning data and handling missing values...\n",
      "Cleaned nearest_neighbours shape: (40, 1097)\n",
      "Preparing final training data...\n",
      "final_training_data shape: (15667, 5)\n",
      "Extracting daily Pm2.5 data..\n",
      " daily_PM_data shape: (40, 1095)\n",
      "Fusing AOD data from multiple years...\n",
      " AOD_final shape: (1350, 1098)\n",
      "Finding nearest neighbours for PM2.5 stations...\n",
      "nearest_neighbours shape: (40, 1098)\n",
      "Cleaning data and handling missing values...\n",
      "Cleaned nearest_neighbours shape: (40, 1097)\n",
      "Preparing final training data...\n",
      "final_training_data shape: (15667, 5)\n",
      "Extracting daily Pm2.5 data..\n",
      " daily_PM_data shape: (40, 1095)\n",
      "Fusing AOD data from multiple years...\n",
      " AOD_final shape: (1350, 1098)\n",
      "Finding nearest neighbours for PM2.5 stations...\n",
      "nearest_neighbours shape: (40, 1098)\n",
      "Cleaning data and handling missing values...\n",
      "Cleaned nearest_neighbours shape: (40, 1097)\n",
      "Preparing final training data...\n",
      "final_training_data shape: (15667, 5)\n",
      "/Users/pavankumar/Documents/Winter_Thesis/Coding_Learning/Master-Thesis\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorboard'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m/Users/pavankumar/Documents/Winter_Thesis/Coding_Learning/Master-Thesis/src/training/trainer_latlon_AOD_PM25.py:267\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[39mNow we have model, loss function, data from here we can build the model trainer that will train the model with the tensor board.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m \n\u001b[1;32m    262\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# ------------ The first step is to devide the data in terms of context and test sets.\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39moptimizer_utils\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m context_target_split\n\u001b[1;32m    269\u001b[0m C_T_data \u001b[39m=\u001b[39m context_target_split(\n\u001b[1;32m    270\u001b[0m     final_data_latlong_AOD_PM25[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m),\n\u001b[1;32m    271\u001b[0m     final_data_latlong_AOD_PM25[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m),\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    274\u001b[0m C_T_data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/Documents/Winter_Thesis/Coding_Learning/Master-Thesis/src/training/optimizer_utils.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtime\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m# tensor board\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtensorboard\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m SummaryWriter\n\u001b[1;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mplt\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m# x_c = torch.tensor([[1, 2, 3], [4, 5, 6]])\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m# y_c = torch.tensor([[7, 8, 9], [10, 11, 12]])\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m# x_t = torch.tensor([[13, 14, 15], [14, 15, 16]])\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m# y_t = torch.tensor([[17, 18, 19], [20, 21, 22]])\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Winter_Thesis/Coding_Learning/Master-Thesis/.venvn/lib/python3.9/site-packages/torch/utils/tensorboard/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorboard\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mdistutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m LooseVersion\n\u001b[1;32m      4\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(tensorboard, \u001b[39m\"\u001b[39m\u001b[39m__version__\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m LooseVersion(\n\u001b[1;32m      5\u001b[0m     tensorboard\u001b[39m.\u001b[39m__version__\n\u001b[1;32m      6\u001b[0m ) \u001b[39m<\u001b[39m LooseVersion(\u001b[39m\"\u001b[39m\u001b[39m1.15\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorboard'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Importing the necessary packages.............\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import arrow\n",
    "from datetime import datetime\n",
    "\n",
    "# from utils.config import config\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "ROOT = Path(__file__).resolve().parents[2]  # tests/ -> project root\n",
    "sys.path.insert(0, str(ROOT))\n",
    "print(ROOT)\n",
    "\n",
    "from configs.utils import config\n",
    "\n",
    "# -------------************---------------------------------\n",
    "# loading all the data loaders here we will load the final data in torch.\n",
    "from Dataloader.Modis_Data_loader.PM25_data_loader_analysis import Modis_data_loader\n",
    "from Dataloader.Modis_Data_loader.PM25_data_loader_analysis import PM_25_dataloader\n",
    "from Dataloader.Modis_Data_loader.PM25_data_loader_analysis import (\n",
    "    combine_the_data_frames,\n",
    ")\n",
    "from Dataloader.Modis_Data_loader.PM25_data_loader_analysis import (\n",
    "    Training_data_loader,\n",
    ")\n",
    "from locationencoder.final_location_encoder import Geospatial_Encoder\n",
    "from Dataloader.Modis_Data_loader.torch_data_loader import (\n",
    "    AirQualityDataset_latlon_AOD_PM25,\n",
    "    AirQualityDataset_latlon_AOD,\n",
    "    AirQualityDataset_latlon_PM25,\n",
    "    AirQualityDataset_latlon,\n",
    ")\n",
    "\n",
    "# importing config files\n",
    "from Dataloader.Modis_Data_loader.final_loader import (\n",
    "    Final_Air_Quality_Dataset_pipeline_latlon_AOD_PM25,\n",
    "    Final_Air_Quality_Dataset_pipeline_latlon_AOD,\n",
    "    Final_Air_Quality_Dataset_pipeline_latlon_PM25,\n",
    "    Final_Air_Quality_Dataset_pipeline_latlon,\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Validation data set \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from Dataloader.Modis_Data_loader.final_loader import (\n",
    "    Final_Air_Quality_Dataset_pipeline_latlon_AOD_PM25_val,\n",
    "    Final_Air_Quality_Dataset_pipeline_latlon_AOD_val,\n",
    "    Final_Air_Quality_Dataset_pipeline_latlon_PM25_val,\n",
    "    Final_Air_Quality_Dataset_pipeline_latlon_val,\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Test data set \n",
    "\"\"\"\n",
    "from Dataloader.Modis_Data_loader.final_loader import (\n",
    "    Final_Air_Quality_Dataset_pipeline_latlon_AOD_PM25_test,\n",
    "    Final_Air_Quality_Dataset_pipeline_latlon_AOD_test,\n",
    "    Final_Air_Quality_Dataset_pipeline_latlon_PM25_test,\n",
    "    Final_Air_Quality_Dataset_pipeline_latlon_test,\n",
    ")\n",
    "\n",
    "# importing the loss function\n",
    "from loss_functions import LossFunctions\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Instances for the training data set \n",
    "\"\"\"\n",
    "# -------------************--------------------------------\n",
    "# Creating all the instance here\n",
    "\n",
    "instance_latlon_AOD_PM25 = Final_Air_Quality_Dataset_pipeline_latlon_AOD_PM25(config)\n",
    "instance_latlon_AOD = Final_Air_Quality_Dataset_pipeline_latlon_AOD(config)\n",
    "instance_latlon_PM25 = Final_Air_Quality_Dataset_pipeline_latlon_PM25(config)\n",
    "intance_latlon = Final_Air_Quality_Dataset_pipeline_latlon(config)\n",
    "\n",
    "\n",
    "# final data with latitude, longitude, AOD and PM2.5\n",
    "instance_latlon_AOD_PM25.modis_data_sets()\n",
    "instance_latlon_AOD_PM25.stations_data_sets()\n",
    "instance_latlon_AOD_PM25.PM_25_data()\n",
    "instance_latlon_AOD_PM25.training_data()\n",
    "instance_latlon_AOD_PM25.Torch_data()\n",
    "final_data_latlong_AOD_PM25 = instance_latlon_AOD_PM25.full_pipeline()\n",
    "len(final_data_latlong_AOD_PM25[0])\n",
    "\n",
    "final_data_latlong_AOD_PM25[1].shape\n",
    "# final data with latitude, longitude and AOD\n",
    "\n",
    "instance_latlon_AOD.modis_data_sets()\n",
    "instance_latlon_AOD.stations_data_sets()\n",
    "instance_latlon_AOD.PM_25_data()\n",
    "instance_latlon_AOD.training_data()\n",
    "instance_latlon_AOD.Torch_data()\n",
    "final_data_latlong_AOD = instance_latlon_AOD.full_pipeline()\n",
    "final_data_latlong_AOD[0]\n",
    "final_data_latlong_AOD[1][0]\n",
    "\n",
    "# final data with latitude, longitude  and PM2.5\n",
    "instance_latlon_PM25.modis_data_sets()\n",
    "instance_latlon_PM25.stations_data_sets()\n",
    "instance_latlon_PM25.PM_25_data()\n",
    "instance_latlon_PM25.training_data()\n",
    "instance_latlon_PM25.Torch_data()\n",
    "final_data_latlong_PM25 = instance_latlon_PM25.full_pipeline()\n",
    "final_data_latlong_PM25[0].shape\n",
    "final_data_latlong_PM25[1]\n",
    "\n",
    "# final data with latitude and longitude\n",
    "intance_latlon.modis_data_sets()\n",
    "intance_latlon.stations_data_sets()\n",
    "intance_latlon.PM_25_data()\n",
    "intance_latlon.training_data()\n",
    "intance_latlon.Torch_data()\n",
    "final_data_latlong = intance_latlon.full_pipeline()\n",
    "final_data_latlong[0].shape\n",
    "final_data_latlong[1][0]\n",
    "\n",
    "\n",
    "# -------------************--------------------------------\n",
    "\"\"\"\n",
    "Final training data.................\n",
    "\"\"\"\n",
    "final_data_latlong_AOD_PM25\n",
    "\n",
    "final_data_latlong_AOD_PM25\n",
    "final_data_latlong_AOD[0].shape\n",
    "final_data_latlong_PM25[0].shape\n",
    "final_data_latlong[0].shape\n",
    "\n",
    "for i, t in enumerate(final_data_latlong_AOD_PM25):\n",
    "    print(i, t.shape)\n",
    "# final data sets wit x, y in torch tensor form\n",
    "final_data_latlong_AOD_PM25[0].shape\n",
    "x, y = final_data_latlong_AOD\n",
    "x.shape\n",
    "final_data_latlong_PM25\n",
    "final_data_latlong\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Instances for the test data \n",
    "\n",
    "\"\"\"\n",
    "instance_latlon_AOD_PM25_val = Final_Air_Quality_Dataset_pipeline_latlon_AOD_PM25_val(\n",
    "    config\n",
    ")\n",
    "instance_latlon_AOD_val = Final_Air_Quality_Dataset_pipeline_latlon_AOD_val(config)\n",
    "instance_latlon_PM25_val = Final_Air_Quality_Dataset_pipeline_latlon_PM25_val(config)\n",
    "instance_latlon_val = Final_Air_Quality_Dataset_pipeline_latlon_val(config)\n",
    "\n",
    "\"\"\"\n",
    "Final test data sets \n",
    "\"\"\"\n",
    "# latlon_AOD_pm25\n",
    "\n",
    "instance_latlon_AOD_PM25_val.modis_data_sets()\n",
    "instance_latlon_AOD_PM25_val.stations_data_sets()\n",
    "instance_latlon_AOD_PM25_val.PM_25_data()\n",
    "instance_latlon_AOD_PM25_val.training_data()\n",
    "instance_latlon_AOD_PM25_val.Torch_data()\n",
    "final_data_latlong_AOD_PM25_val = instance_latlon_AOD_PM25_val.full_pipeline()\n",
    "final_data_latlong_AOD_PM25_val\n",
    "\n",
    "# latlon_AOD\n",
    "instance_latlon_AOD_val.modis_data_sets()\n",
    "instance_latlon_AOD_val.stations_data_sets()\n",
    "instance_latlon_AOD_val.PM_25_data()\n",
    "instance_latlon_AOD_val.training_data()\n",
    "instance_latlon_AOD_val.Torch_data()\n",
    "final_data_latlong_AOD_val = instance_latlon_AOD_val.full_pipeline()\n",
    "final_data_latlong_AOD_val[0].shape\n",
    "\n",
    "\n",
    "# latlon_PM2.5\n",
    "instance_latlon_PM25_val.modis_data_sets()\n",
    "instance_latlon_PM25_val.stations_data_sets()\n",
    "instance_latlon_PM25_val.PM_25_data()\n",
    "instance_latlon_PM25_val.training_data()\n",
    "instance_latlon_PM25_val.Torch_data()\n",
    "final_data_latlong_PM25_val = instance_latlon_PM25_val.full_pipeline()\n",
    "final_data_latlong_PM25_val[0].shape\n",
    "\n",
    "\n",
    "# latlon\n",
    "instance_latlon_val.modis_data_sets()\n",
    "instance_latlon_val.stations_data_sets()\n",
    "instance_latlon_val.PM_25_data()\n",
    "instance_latlon_val.training_data()\n",
    "instance_latlon_val.Torch_data()\n",
    "final_data_latlong_val = instance_latlon_val.full_pipeline()\n",
    "final_data_latlong_val[0].shape\n",
    "\n",
    "# ************************ Final validation data ***************************************\n",
    "final_data_latlong_AOD_PM25_val[0].shape\n",
    "final_data_latlong_AOD_val[0].shape\n",
    "final_data_latlong_PM25_val[0].shape\n",
    "final_data_latlong_val[0].shape\n",
    "# -------------************--------------------------------\n",
    "\n",
    "# training the model in different sdata sets and storing the weights.\n",
    "\n",
    "# -------------************--------------------------------\n",
    "\n",
    "# importing the model from model file.\n",
    "\"\"\"\n",
    "Importing the model................\n",
    "\"\"\"\n",
    "\n",
    "from src.Models.neural_process import NeuralProcess\n",
    "\n",
    "# self, x_c_dim, y_c_dim, x_t_dim, y_t_dim, hidden_dim, latent_dim\n",
    "# self, x_target_dim, z_dim, hidden_dim, y_target_dim\n",
    "# self, x_c_dim, y_c_dim, x_t_dim, y_t_dim, hidden_dim, latent_dim\n",
    "\n",
    "# Creating the models those are compatible with the all kinds of inputs.\n",
    "\n",
    "# -------------************---------------------------------\n",
    "\n",
    "\"\"\"\n",
    "Final models for training........... making attributes from the class\n",
    "\"\"\"\n",
    "\n",
    "model_latlon_AOD_PM25 = NeuralProcess(128, 2, 128, 2, 128, 128)\n",
    "model_latlon_AOD = NeuralProcess(127, 2, 127, 2, 128, 128)\n",
    "model_latlon_PM25 = NeuralProcess(127, 2, 127, 2, 128, 128)\n",
    "model_latlon = NeuralProcess(126, 2, 126, 2, 128, 128)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Here we will import the loss function...........\n",
    "\"\"\"\n",
    "# self, beta, learning_rate, stepsize, Number_of_steps, device#\n",
    "Loss = LossFunctions()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Now we have model, loss function, data from here we can build the model trainer that will train the model with the tensor board.\n",
    "\n",
    "The first & second class --: will devide the data in to chunks and also train, val and test this will create a final data loader. \n",
    "\n",
    "Third class --: this class will create the loop for train the network. \n",
    "\n",
    "Fourth class --: will validate the data set \n",
    "\n",
    "Fifth step --: will evaluate the model.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# ------------ The first step is to devide the data in terms of context and test sets.\n",
    "\n",
    "\n",
    "from optimizer_utils import context_target_split\n",
    "\n",
    "C_T_data = context_target_split(\n",
    "    final_data_latlong_AOD_PM25[0].unsqueeze(0),\n",
    "    final_data_latlong_AOD_PM25[1].unsqueeze(0),\n",
    ")\n",
    "\n",
    "C_T_data[0].shape\n",
    "C_T_data[1].shape\n",
    "C_T_data[2].shape\n",
    "C_T_data[3].shape\n",
    "\n",
    "final_data_latlong_AOD_PM25[0].unsqueeze(0)[0]\n",
    "\n",
    "\"\"\" self,\n",
    "        model,\n",
    "        optimizer,\n",
    "        loss_fn,\n",
    "        device,\n",
    "        log_dir=\"./logs\",\n",
    "        checkpoint_dir=\"./checkpoints\",\n",
    "\"\"\"\n",
    "\n",
    "# Importing the optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# here we are using ADAM optimizer.\n",
    "\n",
    "Optimizer = optim.Adam(model_latlon_AOD_PM25.parameters(), lr=float(config[\"train\"][\"lr\"]))\n",
    "Optimizer_latlong_AOD = optim.Adam(\n",
    "    model_latlon_AOD.parameters(), lr=float(config[\"train\"][\"lr\"])\n",
    ")\n",
    "Optimizer_latlong_PM25 = optim.Adam(\n",
    "    model_latlon_PM25.parameters(), lr=float(config[\"train\"][\"lr\"])\n",
    ")\n",
    "Optimizer_latlong = optim.Adam(model_latlon.parameters(), lr=config.train.lr)\n",
    "\n",
    "# Importing the trainer\n",
    "\n",
    "final_data_latlong_AOD_PM25[0].__getitem__(10)\n",
    "# First data ser priority will be\n",
    "# def train_epoch(self, dataloader, epoch_idx):\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "neural_process_data converts the data from the final paipe line to dataloader format so that we can convert the data and access in the __getitem__ form.\n",
    "\"\"\"\n",
    "\n",
    "from optimizer_utils import neural_process_data\n",
    "# now to start training we need a data-loader that\n",
    "\n",
    "# for train data\n",
    "\n",
    "NeuralProcessData_latlon_AOD_PM25 = neural_process_data(\n",
    "    final_data_latlong_AOD_PM25[0],\n",
    "    final_data_latlong_AOD_PM25[1],\n",
    "    num_points_per_task=200,\n",
    ")\n",
    "\n",
    "# for test data\n",
    "\n",
    "NeuralProcessData_latlon_AOD_PM25_val = neural_process_data(\n",
    "    final_data_latlong_AOD_PM25_val[0],\n",
    "    final_data_latlong_AOD_PM25_val[1],\n",
    "    num_points_per_task=200,\n",
    ")\n",
    "\n",
    "\n",
    "NeuralProcessData_latlon_AOD = neural_process_data(\n",
    "    final_data_latlong_AOD[0],\n",
    "    final_data_latlong_AOD[1],\n",
    "    num_points_per_task=200,\n",
    ")\n",
    "\n",
    "# for test data\n",
    "\n",
    "NeuralProcessData_latlon_AOD_val = neural_process_data(\n",
    "    final_data_latlong_AOD_val[0],\n",
    "    final_data_latlong_AOD_val[1],\n",
    "    num_points_per_task=200,\n",
    ")\n",
    "\n",
    "NeuralProcessData_latlon_PM25 = neural_process_data(\n",
    "    final_data_latlong_PM25[0],\n",
    "    final_data_latlong_PM25[1],\n",
    "    num_points_per_task=200,\n",
    ")\n",
    "\n",
    "# for test data\n",
    "\n",
    "NeuralProcessData_latlon_PM25_val = neural_process_data(\n",
    "    final_data_latlong_PM25_val[0],\n",
    "    final_data_latlong_PM25_val[1],\n",
    "    num_points_per_task=200,\n",
    ")\n",
    "\n",
    "NeuralProcessData_latlon = neural_process_data(\n",
    "    final_data_latlong[0],\n",
    "    final_data_latlong[1],\n",
    "    num_points_per_task=200,\n",
    ")\n",
    "\n",
    "# for test data\n",
    "\n",
    "NeuralProcessData_latlon_val = neural_process_data(\n",
    "    final_data_latlong_val[0],\n",
    "    final_data_latlong_val[1],\n",
    "    num_points_per_task=200,\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------- practice --------------------\n",
    "\n",
    "X_task, Y_task = NeuralProcessData_latlon_AOD[0]\n",
    "X_task.shape\n",
    "Y_task.shape\n",
    "\n",
    "# ------------------------------- practice ----------------------------------\n",
    "\"\"\"\n",
    "Creating the final data loaders for train val and test sets.\n",
    "\"\"\"\n",
    "\n",
    "# ------------------- Here we will start training ---------------------------\n",
    "\n",
    "# creating data loader for training data\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=NeuralProcessData_latlon_AOD_PM25,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "dataloader.dataset\n",
    "\n",
    "dataloader_latlon_AOD = DataLoader(\n",
    "    dataset=NeuralProcessData_latlon_AOD,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "dataloader_latlon_PM25 = DataLoader(\n",
    "    dataset=NeuralProcessData_latlon_PM25,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "dataloader_latlon = DataLoader(\n",
    "    dataset=NeuralProcessData_latlon,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "for x, y in dataloader_latlon_PM25:\n",
    "    x.shape, y.shape\n",
    "\n",
    "for x, y in dataloader_latlon_AOD:\n",
    "    x.shape, y.shape\n",
    "\n",
    "for x, y in dataloader_latlon:\n",
    "    x.shape, y.shape\n",
    "\n",
    "\n",
    "# checking the size and shape of data loader\n",
    "for x, y in dataloader:\n",
    "    print(x.shape, y.shape)\n",
    "len(dataloader)\n",
    "\n",
    "\n",
    "# ---------------------  Creating the validation data loaders --------------------------------\n",
    "\n",
    "\n",
    "# creating data loader fo rvalidation data\n",
    "dataloader_val = DataLoader(\n",
    "    dataset=NeuralProcessData_latlon_AOD_PM25_val,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "\n",
    "dataloader_val_latlon_AOD = DataLoader(\n",
    "    dataset=NeuralProcessData_latlon_AOD_val,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "\n",
    "dataloader_val_latlon_PM25 = DataLoader(\n",
    "    dataset=NeuralProcessData_latlon_PM25_val,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "dataloader_val_latlon = DataLoader(\n",
    "    dataset=NeuralProcessData_latlon_val,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "\n",
    "# checking the size and shape of data loader\n",
    "for x, y in dataloader_val:\n",
    "    print(x.shape, y.shape)\n",
    "len(dataloader_val)\n",
    "\n",
    "\n",
    "for x, y in dataloader_val_latlon_AOD:\n",
    "    x.shape, y.shape\n",
    "\n",
    "\n",
    "for x, y in dataloader_val_latlon_PM25:\n",
    "    x.shape, y.shape\n",
    "\n",
    "for x, y in dataloader_val_latlon:\n",
    "    x.shape, y.shape\n",
    "# In this way we can extract the x, y batches from dataloader.\n",
    "\n",
    "\"\"\"\n",
    "How to check the training dataloader \n",
    "\"\"\"\n",
    "\n",
    "len(dataloader)\n",
    "Xb, Yb = next(iter(dataloader))\n",
    "Xb.shape, Yb.shape\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Start training for all the models in their respective data sets \n",
    "\"\"\"\n",
    "\n",
    "# importing the trainer\n",
    "from optimizer_utils import NPTrainer\n",
    "from optimizer_utils import validation_function\n",
    "\n",
    "# cretaing and saving the logs as per the current training.\n",
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "unique_log_dir = f\"./logs/latlon_AOD_PM25/{current_time}\"\n",
    "unique_checkpoint_dir = f\"/Users/pavankumar/Documents/Winter_Thesis/Coding_Learning/Master-Thesis/checkpoints/latlon_AOD/{current_time}\"\n",
    "\n",
    "## 1 -----------------------  Lat_Lon_AOD_PM2.5 data -----------------------------\n",
    "##################################################################################\n",
    "# Training the model\n",
    "#\n",
    "Training = NPTrainer(\n",
    "    model=model_latlon_AOD_PM25,\n",
    "    optimizer=Optimizer,\n",
    "    loss_fn=Loss,\n",
    "    device=\"cpu\",\n",
    "    log_dir=unique_log_dir,\n",
    "    checkpoint_dir=unique_checkpoint_dir,\n",
    ")\n",
    "\n",
    "# running the epochs.\n",
    "# ... definitions above ...\n",
    "#\n",
    "# Define a \"best loss\" to track improvement\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(10):  # Run for 100 epochs\n",
    "    # 1. TRAIN\n",
    "    train_loss = Training.train_epoch(dataloader, epoch)\n",
    "\n",
    "    # 2. VALIDATE (Call the function we just wrote)\n",
    "    val_loss = validation_function(\n",
    "        model=Training.model,  # Access model from your Trainer class\n",
    "        val_dataloader=dataloader_val,  # You need a separate loader for val data\n",
    "        loss_fn=Loss,\n",
    "        device=\"cpu\",\n",
    "    )\n",
    "\n",
    "    print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # 3. SAVE ONLY IF BETTER\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        Training.save_checkpoint(epoch)\n",
    "        print(f\"   >>> SAVED: New best model found!\")\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "# \"\"\"\n",
    "# Now we need to export the validation data-set so that our trained models can be validated and model with best weights can be selected.\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# self, model, val_dataloader, device, Loss, context_target_split\n",
    "# Now we are extracting the weights and will be using it for the testing purpose we will make our model ready for test.\n",
    "\n",
    "\n",
    "##################################################################\n",
    "# ----------- Training of Lat_Lon_AOD data ------------\n",
    "##################################################################\n",
    "\n",
    "# unique_log_dir = f\"./logs/latlon_AOD/{current_time}\"\n",
    "# unique_checkpoint_dir = f\"./checkpoints/latlon_AOD/{current_time}\"\n",
    "# Training = NPTrainer(\n",
    "#     model=model_latlon_AOD,\n",
    "#     optimizer=Optimizer_latlong_AOD,\n",
    "#     loss_fn=Loss,\n",
    "#     device=\"cpu\",\n",
    "#     log_dir=unique_log_dir,\n",
    "#     checkpoint_dir=unique_checkpoint_dir,\n",
    "# )\n",
    "# #  running the epochs.\n",
    "# #  ... definitions above ...\n",
    "\n",
    "# #  Define a \"best loss\" to track improvement\n",
    "# best_val_loss = float(\"inf\")\n",
    "# #\n",
    "# for epoch in range(100):  # Run for 100 epochs\n",
    "#     # 1. TRAIN\n",
    "#     train_loss = Training.train_epoch(dataloader_latlon_AOD, epoch)\n",
    "#     # 2. VALIDATE (Call the function we just wrote)\n",
    "#     val_loss = validation_function(\n",
    "#         model=model_latlon_AOD,  # Access model from your Trainer class\n",
    "#         val_dataloader=dataloader_val_latlon_AOD,  # You need a separate loader for val data\n",
    "#         loss_fn=Loss,\n",
    "#         device=\"cpu\",\n",
    "#     )\n",
    "#     print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "#     # 3. SAVE ONLY IF BETTER\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         Training.save_checkpoint(epoch)\n",
    "#         print(f\"   >>> SAVED: New best model found!\")\n",
    "\n",
    "\n",
    "# ##################################################################\n",
    "# # -------------Training of Lat_Lon_PM25 data\n",
    "# ##################################################################\n",
    "# current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# unique_log_dir = f\"./logs/latlon_PM25/{current_time}\"\n",
    "# unique_checkpoint_dir = f\"./checkpoints/latlon_PM25/{current_time}\"\n",
    "# Training = NPTrainer(\n",
    "#     model=model_latlon_PM25,\n",
    "#     optimizer=Optimizer_latlong_PM25,\n",
    "#     loss_fn=Loss,\n",
    "#     device=\"cpu\",\n",
    "#     log_dir=unique_log_dir,\n",
    "#     checkpoint_dir=unique_checkpoint_dir,\n",
    "# )\n",
    "# #  running the epochs.\n",
    "# #  ... definitions above ...\n",
    "\n",
    "# #  Define a \"best loss\" to track improvement\n",
    "# best_val_loss = float(\"inf\")\n",
    "# #\n",
    "# for epoch in range(100):  # Run for 100 epochs\n",
    "#     # 1. TRAIN\n",
    "#     train_loss = Training.train_epoch(dataloader_latlon_PM25, epoch)\n",
    "#     # 2. VALIDATE (Call the function we just wrote)\n",
    "#     val_loss = validation_function(\n",
    "#         model=model_latlon_PM25,  # Access model from your Trainer class\n",
    "#         val_dataloader=dataloader_val_latlon_PM25,  # You need a separate loader for val data\n",
    "#         loss_fn=Loss,\n",
    "#         device=\"cpu\",\n",
    "#     )\n",
    "#     print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "#     # 3. SAVE ONLY IF BETTER\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         Training.save_checkpoint(epoch)\n",
    "#         print(f\"   >>> SAVED: New best model found!\")\n",
    "\n",
    "\n",
    "################################################################\n",
    "# Training on lat_lon data set\n",
    "##################################################################\n",
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "unique_log_dir = f\"./logs/latlon/{current_time}\"\n",
    "unique_checkpoint_dir = f\"./checkpoints/latlon/{current_time}\"\n",
    "Training = NPTrainer(\n",
    "    model=model_latlon,\n",
    "    optimizer=Optimizer_latlong,\n",
    "    loss_fn=Loss,\n",
    "    device=\"cpu\",\n",
    "    log_dir=unique_log_dir,\n",
    "    checkpoint_dir=unique_checkpoint_dir,\n",
    ")\n",
    "#  running the epochs.\n",
    "#  ... definitions above ...\n",
    "\n",
    "#  Define a \"best loss\" to track improvement\n",
    "best_val_loss = float(\"inf\")\n",
    "#\n",
    "for epoch in range(10):  # Run for 100 epochs\n",
    "    # 1. TRAIN\n",
    "    train_loss = Training.train_epoch(dataloader_latlon, epoch)\n",
    "    # 2. VALIDATE (Call the function we just wrote)\n",
    "    val_loss = validation_function(\n",
    "        model=Training.model,  # Access model from your Trainer class\n",
    "        val_dataloader=dataloader_val_latlon,  # You need a separate loader for val data\n",
    "        loss_fn=Loss,\n",
    "        device=\"cpu\",\n",
    "    )\n",
    "    print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    # 3. SAVE ONLY IF BETTER\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        Training.save_checkpoint(epoch)\n",
    "        print(f\"   >>> SAVED: New best model found!\")\n",
    "\n",
    "\n",
    "# Loadign the model and then plottign the curvs.............."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e84d649-2468-4082-8654-472e47f8e332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pavankumar/Documents/Winter_Thesis/Coding_Learning/Master-Thesis\n",
      "Extracting daily Pm2.5 data..\n",
      " daily_PM_data shape: (40, 1095)\n",
      "Fusing AOD data from multiple years...\n",
      " AOD_final shape: (1350, 1098)\n",
      "Finding nearest neighbours for PM2.5 stations...\n",
      "nearest_neighbours shape: (40, 1098)\n",
      "Cleaning data and handling missing values...\n",
      "Cleaned nearest_neighbours shape: (40, 1097)\n",
      "Preparing final training data...\n",
      "final_training_data shape: (15667, 5)\n",
      "Extracting daily Pm2.5 data..\n",
      " daily_PM_data shape: (40, 1095)\n",
      "Fusing AOD data from multiple years...\n",
      " AOD_final shape: (1350, 1098)\n",
      "Finding nearest neighbours for PM2.5 stations...\n",
      "nearest_neighbours shape: (40, 1098)\n",
      "Cleaning data and handling missing values...\n",
      "Cleaned nearest_neighbours shape: (40, 1097)\n",
      "Preparing final training data...\n",
      "final_training_data shape: (15667, 5)\n",
      "Extracting daily Pm2.5 data..\n",
      " daily_PM_data shape: (40, 1095)\n",
      "Fusing AOD data from multiple years...\n",
      " AOD_final shape: (1350, 1098)\n",
      "Finding nearest neighbours for PM2.5 stations...\n",
      "nearest_neighbours shape: (40, 1098)\n",
      "Cleaning data and handling missing values...\n",
      "Cleaned nearest_neighbours shape: (40, 1097)\n",
      "Preparing final training data...\n",
      "final_training_data shape: (15667, 5)\n",
      "Extracting daily Pm2.5 data..\n",
      " daily_PM_data shape: (40, 1095)\n",
      "Fusing AOD data from multiple years...\n",
      " AOD_final shape: (1350, 1098)\n",
      "Finding nearest neighbours for PM2.5 stations...\n",
      "nearest_neighbours shape: (40, 1098)\n",
      "Cleaning data and handling missing values...\n",
      "Cleaned nearest_neighbours shape: (40, 1097)\n",
      "Preparing final training data...\n",
      "final_training_data shape: (15667, 5)\n",
      "0 torch.Size([12307, 128])\n",
      "1 torch.Size([12307, 2])\n",
      "Extracting daily Pm2.5 data..\n",
      " daily_PM_data shape: (40, 1095)\n",
      "Fusing AOD data from multiple years...\n",
      " AOD_final shape: (1350, 1098)\n",
      "Finding nearest neighbours for PM2.5 stations...\n",
      "nearest_neighbours shape: (40, 1098)\n",
      "Cleaning data and handling missing values...\n",
      "Cleaned nearest_neighbours shape: (40, 1097)\n",
      "Preparing final training data...\n",
      "final_training_data shape: (15667, 5)\n",
      "Extracting daily Pm2.5 data..\n",
      " daily_PM_data shape: (40, 1095)\n",
      "Fusing AOD data from multiple years...\n",
      " AOD_final shape: (1350, 1098)\n",
      "Finding nearest neighbours for PM2.5 stations...\n",
      "nearest_neighbours shape: (40, 1098)\n",
      "Cleaning data and handling missing values...\n",
      "Cleaned nearest_neighbours shape: (40, 1097)\n",
      "Preparing final training data...\n",
      "final_training_data shape: (15667, 5)\n",
      "Extracting daily Pm2.5 data..\n",
      " daily_PM_data shape: (40, 1095)\n",
      "Fusing AOD data from multiple years...\n",
      " AOD_final shape: (1350, 1098)\n",
      "Finding nearest neighbours for PM2.5 stations...\n",
      "nearest_neighbours shape: (40, 1098)\n",
      "Cleaning data and handling missing values...\n",
      "Cleaned nearest_neighbours shape: (40, 1097)\n",
      "Preparing final training data...\n",
      "final_training_data shape: (15667, 5)\n",
      "Extracting daily Pm2.5 data..\n",
      " daily_PM_data shape: (40, 1095)\n",
      "Fusing AOD data from multiple years...\n",
      " AOD_final shape: (1350, 1098)\n",
      "Finding nearest neighbours for PM2.5 stations...\n",
      "nearest_neighbours shape: (40, 1098)\n",
      "Cleaning data and handling missing values...\n",
      "Cleaned nearest_neighbours shape: (40, 1097)\n",
      "Preparing final training data...\n",
      "final_training_data shape: (15667, 5)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m/Users/pavankumar/Documents/Winter_Thesis/Coding_Learning/Master-Thesis/src/training/trainer_latlon_AOD_PM25.py:296\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39moptim\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[39m# here we are using ADAM optimizer.\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m Optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39;49mAdam(model_latlon_AOD_PM25\u001b[39m.\u001b[39;49mparameters(), lr\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    297\u001b[0m Optimizer_latlong_AOD \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(\n\u001b[1;32m    298\u001b[0m     model_latlon_AOD\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    299\u001b[0m )\n\u001b[1;32m    300\u001b[0m Optimizer_latlong_PM25 \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(\n\u001b[1;32m    301\u001b[0m     model_latlon_PM25\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    302\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Winter_Thesis/Coding_Learning/Master-Thesis/.venvn/lib/python3.9/site-packages/torch/optim/adam.py:18\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, params, lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m, betas\u001b[39m=\u001b[39m(\u001b[39m0.9\u001b[39m, \u001b[39m0.999\u001b[39m), eps\u001b[39m=\u001b[39m\u001b[39m1e-8\u001b[39m,\n\u001b[1;32m     15\u001b[0m              weight_decay\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, amsgrad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m, foreach: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m              maximize: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, capturable: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m              differentiable: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, fused: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 18\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m0.0\u001b[39;49m \u001b[39m<\u001b[39;49m\u001b[39m=\u001b[39;49m lr:\n\u001b[1;32m     19\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid learning rate: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(lr))\n\u001b[1;32m     20\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m0.0\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m eps:\n",
      "\u001b[0;31mTypeError\u001b[0m: '<=' not supported between instances of 'float' and 'str'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Importing the necessary packages.............\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import arrow\n",
    "from datetime import datetime\n",
    "\n",
    "# from utils.config import config\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "ROOT = Path(__file__).resolve().parents[2]  # tests/ -> project root\n",
    "sys.path.insert(0, str(ROOT))\n",
    "print(ROOT)\n",
    "\n",
    "from configs.utils import config\n",
    "\n",
    "# -------------************---------------------------------\n",
    "# loading all the data loaders here we will load the final data in torch.\n",
    "from Dataloader.Modis_Data_loader.PM25_data_loader_analysis import Modis_data_loader\n",
    "from Dataloader.Modis_Data_loader.PM25_data_loader_analysis import PM_25_dataloader\n",
    "from Dataloader.Modis_Data_loader.PM25_data_loader_analysis import (\n",
    "    combine_the_data_frames,\n",
    ")\n",
    "from Dataloader.Modis_Data_loader.PM25_data_loader_analysis import (\n",
    "    Training_data_loader,\n",
    ")\n",
    "from locationencoder.final_location_encoder import Geospatial_Encoder\n",
    "from Dataloader.Modis_Data_loader.torch_data_loader import (\n",
    "    AirQualityDataset_latlon_AOD_PM25,\n",
    "    AirQualityDataset_latlon_AOD,\n",
    "    AirQualityDataset_latlon_PM25,\n",
    "    AirQualityDataset_latlon,\n",
    ")\n",
    "\n",
    "# importing config files\n",
    "from Dataloader.Modis_Data_loader.final_loader import (\n",
    "    Final_Air_Quality_Dataset_pipeline_latlon_AOD_PM25,\n",
    "    Final_Air_Quality_Dataset_pipeline_latlon_AOD,\n",
    "    Final_Air_Quality_Dataset_pipeline_latlon_PM25,\n",
    "    Final_Air_Quality_Dataset_pipeline_latlon,\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Validation data set \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from Dataloader.Modis_Data_loader.final_loader import (\n",
    "    Final_Air_Quality_Dataset_pipeline_latlon_AOD_PM25_val,\n",
    "    Final_Air_Quality_Dataset_pipeline_latlon_AOD_val,\n",
    "    Final_Air_Quality_Dataset_pipeline_latlon_PM25_val,\n",
    "    Final_Air_Quality_Dataset_pipeline_latlon_val,\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Test data set \n",
    "\"\"\"\n",
    "from Dataloader.Modis_Data_loader.final_loader import (\n",
    "    Final_Air_Quality_Dataset_pipeline_latlon_AOD_PM25_test,\n",
    "    Final_Air_Quality_Dataset_pipeline_latlon_AOD_test,\n",
    "    Final_Air_Quality_Dataset_pipeline_latlon_PM25_test,\n",
    "    Final_Air_Quality_Dataset_pipeline_latlon_test,\n",
    ")\n",
    "\n",
    "# importing the loss function\n",
    "from loss_functions import LossFunctions\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Instances for the training data set \n",
    "\"\"\"\n",
    "# -------------************--------------------------------\n",
    "# Creating all the instance here\n",
    "\n",
    "instance_latlon_AOD_PM25 = Final_Air_Quality_Dataset_pipeline_latlon_AOD_PM25(config)\n",
    "instance_latlon_AOD = Final_Air_Quality_Dataset_pipeline_latlon_AOD(config)\n",
    "instance_latlon_PM25 = Final_Air_Quality_Dataset_pipeline_latlon_PM25(config)\n",
    "intance_latlon = Final_Air_Quality_Dataset_pipeline_latlon(config)\n",
    "\n",
    "\n",
    "# final data with latitude, longitude, AOD and PM2.5\n",
    "instance_latlon_AOD_PM25.modis_data_sets()\n",
    "instance_latlon_AOD_PM25.stations_data_sets()\n",
    "instance_latlon_AOD_PM25.PM_25_data()\n",
    "instance_latlon_AOD_PM25.training_data()\n",
    "instance_latlon_AOD_PM25.Torch_data()\n",
    "final_data_latlong_AOD_PM25 = instance_latlon_AOD_PM25.full_pipeline()\n",
    "len(final_data_latlong_AOD_PM25[0])\n",
    "\n",
    "final_data_latlong_AOD_PM25[1].shape\n",
    "# final data with latitude, longitude and AOD\n",
    "\n",
    "instance_latlon_AOD.modis_data_sets()\n",
    "instance_latlon_AOD.stations_data_sets()\n",
    "instance_latlon_AOD.PM_25_data()\n",
    "instance_latlon_AOD.training_data()\n",
    "instance_latlon_AOD.Torch_data()\n",
    "final_data_latlong_AOD = instance_latlon_AOD.full_pipeline()\n",
    "final_data_latlong_AOD[0]\n",
    "final_data_latlong_AOD[1][0]\n",
    "\n",
    "# final data with latitude, longitude  and PM2.5\n",
    "instance_latlon_PM25.modis_data_sets()\n",
    "instance_latlon_PM25.stations_data_sets()\n",
    "instance_latlon_PM25.PM_25_data()\n",
    "instance_latlon_PM25.training_data()\n",
    "instance_latlon_PM25.Torch_data()\n",
    "final_data_latlong_PM25 = instance_latlon_PM25.full_pipeline()\n",
    "final_data_latlong_PM25[0].shape\n",
    "final_data_latlong_PM25[1]\n",
    "\n",
    "# final data with latitude and longitude\n",
    "intance_latlon.modis_data_sets()\n",
    "intance_latlon.stations_data_sets()\n",
    "intance_latlon.PM_25_data()\n",
    "intance_latlon.training_data()\n",
    "intance_latlon.Torch_data()\n",
    "final_data_latlong = intance_latlon.full_pipeline()\n",
    "final_data_latlong[0].shape\n",
    "final_data_latlong[1][0]\n",
    "\n",
    "\n",
    "# -------------************--------------------------------\n",
    "\"\"\"\n",
    "Final training data.................\n",
    "\"\"\"\n",
    "final_data_latlong_AOD_PM25\n",
    "\n",
    "final_data_latlong_AOD_PM25\n",
    "final_data_latlong_AOD[0].shape\n",
    "final_data_latlong_PM25[0].shape\n",
    "final_data_latlong[0].shape\n",
    "\n",
    "for i, t in enumerate(final_data_latlong_AOD_PM25):\n",
    "    print(i, t.shape)\n",
    "# final data sets wit x, y in torch tensor form\n",
    "final_data_latlong_AOD_PM25[0].shape\n",
    "x, y = final_data_latlong_AOD\n",
    "x.shape\n",
    "final_data_latlong_PM25\n",
    "final_data_latlong\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Instances for the test data \n",
    "\n",
    "\"\"\"\n",
    "instance_latlon_AOD_PM25_val = Final_Air_Quality_Dataset_pipeline_latlon_AOD_PM25_val(\n",
    "    config\n",
    ")\n",
    "instance_latlon_AOD_val = Final_Air_Quality_Dataset_pipeline_latlon_AOD_val(config)\n",
    "instance_latlon_PM25_val = Final_Air_Quality_Dataset_pipeline_latlon_PM25_val(config)\n",
    "instance_latlon_val = Final_Air_Quality_Dataset_pipeline_latlon_val(config)\n",
    "\n",
    "\"\"\"\n",
    "Final test data sets \n",
    "\"\"\"\n",
    "# latlon_AOD_pm25\n",
    "\n",
    "instance_latlon_AOD_PM25_val.modis_data_sets()\n",
    "instance_latlon_AOD_PM25_val.stations_data_sets()\n",
    "instance_latlon_AOD_PM25_val.PM_25_data()\n",
    "instance_latlon_AOD_PM25_val.training_data()\n",
    "instance_latlon_AOD_PM25_val.Torch_data()\n",
    "final_data_latlong_AOD_PM25_val = instance_latlon_AOD_PM25_val.full_pipeline()\n",
    "final_data_latlong_AOD_PM25_val\n",
    "\n",
    "# latlon_AOD\n",
    "instance_latlon_AOD_val.modis_data_sets()\n",
    "instance_latlon_AOD_val.stations_data_sets()\n",
    "instance_latlon_AOD_val.PM_25_data()\n",
    "instance_latlon_AOD_val.training_data()\n",
    "instance_latlon_AOD_val.Torch_data()\n",
    "final_data_latlong_AOD_val = instance_latlon_AOD_val.full_pipeline()\n",
    "final_data_latlong_AOD_val[0].shape\n",
    "\n",
    "\n",
    "# latlon_PM2.5\n",
    "instance_latlon_PM25_val.modis_data_sets()\n",
    "instance_latlon_PM25_val.stations_data_sets()\n",
    "instance_latlon_PM25_val.PM_25_data()\n",
    "instance_latlon_PM25_val.training_data()\n",
    "instance_latlon_PM25_val.Torch_data()\n",
    "final_data_latlong_PM25_val = instance_latlon_PM25_val.full_pipeline()\n",
    "final_data_latlong_PM25_val[0].shape\n",
    "\n",
    "\n",
    "# latlon\n",
    "instance_latlon_val.modis_data_sets()\n",
    "instance_latlon_val.stations_data_sets()\n",
    "instance_latlon_val.PM_25_data()\n",
    "instance_latlon_val.training_data()\n",
    "instance_latlon_val.Torch_data()\n",
    "final_data_latlong_val = instance_latlon_val.full_pipeline()\n",
    "final_data_latlong_val[0].shape\n",
    "\n",
    "# ************************ Final validation data ***************************************\n",
    "final_data_latlong_AOD_PM25_val[0].shape\n",
    "final_data_latlong_AOD_val[0].shape\n",
    "final_data_latlong_PM25_val[0].shape\n",
    "final_data_latlong_val[0].shape\n",
    "# -------------************--------------------------------\n",
    "\n",
    "# training the model in different sdata sets and storing the weights.\n",
    "\n",
    "# -------------************--------------------------------\n",
    "\n",
    "# importing the model from model file.\n",
    "\"\"\"\n",
    "Importing the model................\n",
    "\"\"\"\n",
    "\n",
    "from src.Models.neural_process import NeuralProcess\n",
    "\n",
    "# self, x_c_dim, y_c_dim, x_t_dim, y_t_dim, hidden_dim, latent_dim\n",
    "# self, x_target_dim, z_dim, hidden_dim, y_target_dim\n",
    "# self, x_c_dim, y_c_dim, x_t_dim, y_t_dim, hidden_dim, latent_dim\n",
    "\n",
    "# Creating the models those are compatible with the all kinds of inputs.\n",
    "\n",
    "# -------------************---------------------------------\n",
    "\n",
    "\"\"\"\n",
    "Final models for training........... making attributes from the class\n",
    "\"\"\"\n",
    "\n",
    "model_latlon_AOD_PM25 = NeuralProcess(128, 2, 128, 2, 128, 128)\n",
    "model_latlon_AOD = NeuralProcess(127, 2, 127, 2, 128, 128)\n",
    "model_latlon_PM25 = NeuralProcess(127, 2, 127, 2, 128, 128)\n",
    "model_latlon = NeuralProcess(126, 2, 126, 2, 128, 128)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Here we will import the loss function...........\n",
    "\"\"\"\n",
    "# self, beta, learning_rate, stepsize, Number_of_steps, device#\n",
    "Loss = LossFunctions()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Now we have model, loss function, data from here we can build the model trainer that will train the model with the tensor board.\n",
    "\n",
    "The first & second class --: will devide the data in to chunks and also train, val and test this will create a final data loader. \n",
    "\n",
    "Third class --: this class will create the loop for train the network. \n",
    "\n",
    "Fourth class --: will validate the data set \n",
    "\n",
    "Fifth step --: will evaluate the model.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# ------------ The first step is to devide the data in terms of context and test sets.\n",
    "\n",
    "\n",
    "from optimizer_utils import context_target_split\n",
    "\n",
    "C_T_data = context_target_split(\n",
    "    final_data_latlong_AOD_PM25[0].unsqueeze(0),\n",
    "    final_data_latlong_AOD_PM25[1].unsqueeze(0),\n",
    ")\n",
    "\n",
    "C_T_data[0].shape\n",
    "C_T_data[1].shape\n",
    "C_T_data[2].shape\n",
    "C_T_data[3].shape\n",
    "\n",
    "final_data_latlong_AOD_PM25[0].unsqueeze(0)[0]\n",
    "\n",
    "\"\"\" self,\n",
    "        model,\n",
    "        optimizer,\n",
    "        loss_fn,\n",
    "        device,\n",
    "        log_dir=\"./logs\",\n",
    "        checkpoint_dir=\"./checkpoints\",\n",
    "\"\"\"\n",
    "\n",
    "# Importing the optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# here we are using ADAM optimizer.\n",
    "\n",
    "Optimizer = optim.Adam(model_latlon_AOD_PM25.parameters(), lr=float(config[\"train\"][\"lr\"]))\n",
    "Optimizer_latlong_AOD = optim.Adam(\n",
    "    model_latlon_AOD.parameters(), lr=float(config[\"train\"][\"lr\"])\n",
    ")\n",
    "Optimizer_latlong_PM25 = optim.Adam(\n",
    "    model_latlon_PM25.parameters(), lr=float(config[\"train\"][\"lr\"])\n",
    ")\n",
    "Optimizer_latlong = optim.Adam(model_latlon.parameters(), lr=config.train.lr)\n",
    "\n",
    "# Importing the trainer\n",
    "\n",
    "final_data_latlong_AOD_PM25[0].__getitem__(10)\n",
    "# First data ser priority will be\n",
    "# def train_epoch(self, dataloader, epoch_idx):\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "neural_process_data converts the data from the final paipe line to dataloader format so that we can convert the data and access in the __getitem__ form.\n",
    "\"\"\"\n",
    "\n",
    "from optimizer_utils import neural_process_data\n",
    "# now to start training we need a data-loader that\n",
    "\n",
    "# for train data\n",
    "\n",
    "NeuralProcessData_latlon_AOD_PM25 = neural_process_data(\n",
    "    final_data_latlong_AOD_PM25[0],\n",
    "    final_data_latlong_AOD_PM25[1],\n",
    "    num_points_per_task=200,\n",
    ")\n",
    "\n",
    "# for test data\n",
    "\n",
    "NeuralProcessData_latlon_AOD_PM25_val = neural_process_data(\n",
    "    final_data_latlong_AOD_PM25_val[0],\n",
    "    final_data_latlong_AOD_PM25_val[1],\n",
    "    num_points_per_task=200,\n",
    ")\n",
    "\n",
    "\n",
    "NeuralProcessData_latlon_AOD = neural_process_data(\n",
    "    final_data_latlong_AOD[0],\n",
    "    final_data_latlong_AOD[1],\n",
    "    num_points_per_task=200,\n",
    ")\n",
    "\n",
    "# for test data\n",
    "\n",
    "NeuralProcessData_latlon_AOD_val = neural_process_data(\n",
    "    final_data_latlong_AOD_val[0],\n",
    "    final_data_latlong_AOD_val[1],\n",
    "    num_points_per_task=200,\n",
    ")\n",
    "\n",
    "NeuralProcessData_latlon_PM25 = neural_process_data(\n",
    "    final_data_latlong_PM25[0],\n",
    "    final_data_latlong_PM25[1],\n",
    "    num_points_per_task=200,\n",
    ")\n",
    "\n",
    "# for test data\n",
    "\n",
    "NeuralProcessData_latlon_PM25_val = neural_process_data(\n",
    "    final_data_latlong_PM25_val[0],\n",
    "    final_data_latlong_PM25_val[1],\n",
    "    num_points_per_task=200,\n",
    ")\n",
    "\n",
    "NeuralProcessData_latlon = neural_process_data(\n",
    "    final_data_latlong[0],\n",
    "    final_data_latlong[1],\n",
    "    num_points_per_task=200,\n",
    ")\n",
    "\n",
    "# for test data\n",
    "\n",
    "NeuralProcessData_latlon_val = neural_process_data(\n",
    "    final_data_latlong_val[0],\n",
    "    final_data_latlong_val[1],\n",
    "    num_points_per_task=200,\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------- practice --------------------\n",
    "\n",
    "X_task, Y_task = NeuralProcessData_latlon_AOD[0]\n",
    "X_task.shape\n",
    "Y_task.shape\n",
    "\n",
    "# ------------------------------- practice ----------------------------------\n",
    "\"\"\"\n",
    "Creating the final data loaders for train val and test sets.\n",
    "\"\"\"\n",
    "\n",
    "# ------------------- Here we will start training ---------------------------\n",
    "\n",
    "# creating data loader for training data\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=NeuralProcessData_latlon_AOD_PM25,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "dataloader.dataset\n",
    "\n",
    "dataloader_latlon_AOD = DataLoader(\n",
    "    dataset=NeuralProcessData_latlon_AOD,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "dataloader_latlon_PM25 = DataLoader(\n",
    "    dataset=NeuralProcessData_latlon_PM25,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "dataloader_latlon = DataLoader(\n",
    "    dataset=NeuralProcessData_latlon,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "for x, y in dataloader_latlon_PM25:\n",
    "    x.shape, y.shape\n",
    "\n",
    "for x, y in dataloader_latlon_AOD:\n",
    "    x.shape, y.shape\n",
    "\n",
    "for x, y in dataloader_latlon:\n",
    "    x.shape, y.shape\n",
    "\n",
    "\n",
    "# checking the size and shape of data loader\n",
    "for x, y in dataloader:\n",
    "    print(x.shape, y.shape)\n",
    "len(dataloader)\n",
    "\n",
    "\n",
    "# ---------------------  Creating the validation data loaders --------------------------------\n",
    "\n",
    "\n",
    "# creating data loader fo rvalidation data\n",
    "dataloader_val = DataLoader(\n",
    "    dataset=NeuralProcessData_latlon_AOD_PM25_val,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "\n",
    "dataloader_val_latlon_AOD = DataLoader(\n",
    "    dataset=NeuralProcessData_latlon_AOD_val,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "\n",
    "dataloader_val_latlon_PM25 = DataLoader(\n",
    "    dataset=NeuralProcessData_latlon_PM25_val,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "dataloader_val_latlon = DataLoader(\n",
    "    dataset=NeuralProcessData_latlon_val,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "\n",
    "# checking the size and shape of data loader\n",
    "for x, y in dataloader_val:\n",
    "    print(x.shape, y.shape)\n",
    "len(dataloader_val)\n",
    "\n",
    "\n",
    "for x, y in dataloader_val_latlon_AOD:\n",
    "    x.shape, y.shape\n",
    "\n",
    "\n",
    "for x, y in dataloader_val_latlon_PM25:\n",
    "    x.shape, y.shape\n",
    "\n",
    "for x, y in dataloader_val_latlon:\n",
    "    x.shape, y.shape\n",
    "# In this way we can extract the x, y batches from dataloader.\n",
    "\n",
    "\"\"\"\n",
    "How to check the training dataloader \n",
    "\"\"\"\n",
    "\n",
    "len(dataloader)\n",
    "Xb, Yb = next(iter(dataloader))\n",
    "Xb.shape, Yb.shape\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Start training for all the models in their respective data sets \n",
    "\"\"\"\n",
    "\n",
    "# importing the trainer\n",
    "from optimizer_utils import NPTrainer\n",
    "from optimizer_utils import validation_function\n",
    "\n",
    "# cretaing and saving the logs as per the current training.\n",
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "unique_log_dir = f\"./logs/latlon_AOD_PM25/{current_time}\"\n",
    "unique_checkpoint_dir = f\"/Users/pavankumar/Documents/Winter_Thesis/Coding_Learning/Master-Thesis/checkpoints/latlon_AOD/{current_time}\"\n",
    "\n",
    "## 1 -----------------------  Lat_Lon_AOD_PM2.5 data -----------------------------\n",
    "##################################################################################\n",
    "# Training the model\n",
    "#\n",
    "Training = NPTrainer(\n",
    "    model=model_latlon_AOD_PM25,\n",
    "    optimizer=Optimizer,\n",
    "    loss_fn=Loss,\n",
    "    device=\"cpu\",\n",
    "    log_dir=unique_log_dir,\n",
    "    checkpoint_dir=unique_checkpoint_dir,\n",
    ")\n",
    "\n",
    "# running the epochs.\n",
    "# ... definitions above ...\n",
    "#\n",
    "# Define a \"best loss\" to track improvement\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(10):  # Run for 100 epochs\n",
    "    # 1. TRAIN\n",
    "    train_loss = Training.train_epoch(dataloader, epoch)\n",
    "\n",
    "    # 2. VALIDATE (Call the function we just wrote)\n",
    "    val_loss = validation_function(\n",
    "        model=Training.model,  # Access model from your Trainer class\n",
    "        val_dataloader=dataloader_val,  # You need a separate loader for val data\n",
    "        loss_fn=Loss,\n",
    "        device=\"cpu\",\n",
    "    )\n",
    "\n",
    "    print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # 3. SAVE ONLY IF BETTER\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        Training.save_checkpoint(epoch)\n",
    "        print(f\"   >>> SAVED: New best model found!\")\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "# \"\"\"\n",
    "# Now we need to export the validation data-set so that our trained models can be validated and model with best weights can be selected.\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# self, model, val_dataloader, device, Loss, context_target_split\n",
    "# Now we are extracting the weights and will be using it for the testing purpose we will make our model ready for test.\n",
    "\n",
    "\n",
    "##################################################################\n",
    "# ----------- Training of Lat_Lon_AOD data ------------\n",
    "##################################################################\n",
    "\n",
    "# unique_log_dir = f\"./logs/latlon_AOD/{current_time}\"\n",
    "# unique_checkpoint_dir = f\"./checkpoints/latlon_AOD/{current_time}\"\n",
    "# Training = NPTrainer(\n",
    "#     model=model_latlon_AOD,\n",
    "#     optimizer=Optimizer_latlong_AOD,\n",
    "#     loss_fn=Loss,\n",
    "#     device=\"cpu\",\n",
    "#     log_dir=unique_log_dir,\n",
    "#     checkpoint_dir=unique_checkpoint_dir,\n",
    "# )\n",
    "# #  running the epochs.\n",
    "# #  ... definitions above ...\n",
    "\n",
    "# #  Define a \"best loss\" to track improvement\n",
    "# best_val_loss = float(\"inf\")\n",
    "# #\n",
    "# for epoch in range(100):  # Run for 100 epochs\n",
    "#     # 1. TRAIN\n",
    "#     train_loss = Training.train_epoch(dataloader_latlon_AOD, epoch)\n",
    "#     # 2. VALIDATE (Call the function we just wrote)\n",
    "#     val_loss = validation_function(\n",
    "#         model=model_latlon_AOD,  # Access model from your Trainer class\n",
    "#         val_dataloader=dataloader_val_latlon_AOD,  # You need a separate loader for val data\n",
    "#         loss_fn=Loss,\n",
    "#         device=\"cpu\",\n",
    "#     )\n",
    "#     print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "#     # 3. SAVE ONLY IF BETTER\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         Training.save_checkpoint(epoch)\n",
    "#         print(f\"   >>> SAVED: New best model found!\")\n",
    "\n",
    "\n",
    "# ##################################################################\n",
    "# # -------------Training of Lat_Lon_PM25 data\n",
    "# ##################################################################\n",
    "# current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# unique_log_dir = f\"./logs/latlon_PM25/{current_time}\"\n",
    "# unique_checkpoint_dir = f\"./checkpoints/latlon_PM25/{current_time}\"\n",
    "# Training = NPTrainer(\n",
    "#     model=model_latlon_PM25,\n",
    "#     optimizer=Optimizer_latlong_PM25,\n",
    "#     loss_fn=Loss,\n",
    "#     device=\"cpu\",\n",
    "#     log_dir=unique_log_dir,\n",
    "#     checkpoint_dir=unique_checkpoint_dir,\n",
    "# )\n",
    "# #  running the epochs.\n",
    "# #  ... definitions above ...\n",
    "\n",
    "# #  Define a \"best loss\" to track improvement\n",
    "# best_val_loss = float(\"inf\")\n",
    "# #\n",
    "# for epoch in range(100):  # Run for 100 epochs\n",
    "#     # 1. TRAIN\n",
    "#     train_loss = Training.train_epoch(dataloader_latlon_PM25, epoch)\n",
    "#     # 2. VALIDATE (Call the function we just wrote)\n",
    "#     val_loss = validation_function(\n",
    "#         model=model_latlon_PM25,  # Access model from your Trainer class\n",
    "#         val_dataloader=dataloader_val_latlon_PM25,  # You need a separate loader for val data\n",
    "#         loss_fn=Loss,\n",
    "#         device=\"cpu\",\n",
    "#     )\n",
    "#     print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "#     # 3. SAVE ONLY IF BETTER\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         Training.save_checkpoint(epoch)\n",
    "#         print(f\"   >>> SAVED: New best model found!\")\n",
    "\n",
    "\n",
    "################################################################\n",
    "# Training on lat_lon data set\n",
    "##################################################################\n",
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "unique_log_dir = f\"./logs/latlon/{current_time}\"\n",
    "unique_checkpoint_dir = f\"./checkpoints/latlon/{current_time}\"\n",
    "Training = NPTrainer(\n",
    "    model=model_latlon,\n",
    "    optimizer=Optimizer_latlong,\n",
    "    loss_fn=Loss,\n",
    "    device=\"cpu\",\n",
    "    log_dir=unique_log_dir,\n",
    "    checkpoint_dir=unique_checkpoint_dir,\n",
    ")\n",
    "#  running the epochs.\n",
    "#  ... definitions above ...\n",
    "\n",
    "#  Define a \"best loss\" to track improvement\n",
    "best_val_loss = float(\"inf\")\n",
    "#\n",
    "for epoch in range(10):  # Run for 100 epochs\n",
    "    # 1. TRAIN\n",
    "    train_loss = Training.train_epoch(dataloader_latlon, epoch)\n",
    "    # 2. VALIDATE (Call the function we just wrote)\n",
    "    val_loss = validation_function(\n",
    "        model=Training.model,  # Access model from your Trainer class\n",
    "        val_dataloader=dataloader_val_latlon,  # You need a separate loader for val data\n",
    "        loss_fn=Loss,\n",
    "        device=\"cpu\",\n",
    "    )\n",
    "    print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    # 3. SAVE ONLY IF BETTER\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        Training.save_checkpoint(epoch)\n",
    "        print(f\"   >>> SAVED: New best model found!\")\n",
    "\n",
    "\n",
    "# Loadign the model and then plottign the curvs.............."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9275a3-d42f-419a-8b4f-468c09ee1705",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader_val_latlon' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m/Users/pavankumar/Documents/Winter_Thesis/Coding_Learning/Master-Thesis/src/training/trainer_latlon_AOD_PM25.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m dataloader_val_latlon:\n\u001b[1;32m      2\u001b[0m     x\u001b[39m.\u001b[39mshape, y\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader_val_latlon' is not defined"
     ]
    }
   ],
   "source": [
    "for x, y in dataloader_val_latlon:\n",
    "    x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b152fc-6201-4559-bbdf-07aa64161116",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m/Users/pavankumar/Documents/Winter_Thesis/Coding_Learning/Master-Thesis/src/training/trainer_latlon_AOD_PM25.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39;49mAdam(model_latlon_AOD_PM25\u001b[39m.\u001b[39;49mparameters(), lr\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m~/Documents/Winter_Thesis/Coding_Learning/Master-Thesis/.venvn/lib/python3.9/site-packages/torch/optim/adam.py:18\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, params, lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m, betas\u001b[39m=\u001b[39m(\u001b[39m0.9\u001b[39m, \u001b[39m0.999\u001b[39m), eps\u001b[39m=\u001b[39m\u001b[39m1e-8\u001b[39m,\n\u001b[1;32m     15\u001b[0m              weight_decay\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, amsgrad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m, foreach: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m              maximize: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, capturable: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m              differentiable: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, fused: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 18\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m0.0\u001b[39;49m \u001b[39m<\u001b[39;49m\u001b[39m=\u001b[39;49m lr:\n\u001b[1;32m     19\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid learning rate: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(lr))\n\u001b[1;32m     20\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m0.0\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m eps:\n",
      "\u001b[0;31mTypeError\u001b[0m: '<=' not supported between instances of 'float' and 'str'"
     ]
    }
   ],
   "source": [
    "Optimizer = optim.Adam(model_latlon_AOD_PM25.parameters(), lr=float(config[\"train\"][\"lr\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa52939-f3ef-47b6-ba31-dd74ef850d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba374d5-43a0-4a5e-8f5c-ee833aac21ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3633, -1.0528, -0.7438,  ...,  0.5218,  0.9654,  1.2592],\n",
       "        [-1.3633, -1.0528, -0.7438,  ...,  0.5218, -0.7220, -0.0482],\n",
       "        [-1.3633, -1.0528, -0.7438,  ...,  0.5218, -0.3033, -0.1231],\n",
       "        ...,\n",
       "        [-1.3623, -1.0516, -0.7432,  ...,  0.5223, -0.5858, -0.2545],\n",
       "        [-1.3623, -1.0516, -0.7432,  ...,  0.5223,  0.4649, -0.3712],\n",
       "        [-1.3623, -1.0516, -0.7432,  ...,  0.5223, -0.4296,  0.0278]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data_latlong_AOD_PM25[0].unsqueeze(0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def41731-2a2d-4e3a-8643-3d7b6738434b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m/Users/pavankumar/Documents/Winter_Thesis/Coding_Learning/Master-Thesis/src/training/trainer_latlon_AOD_PM25.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39;49mAdam(model_latlon_AOD_PM25\u001b[39m.\u001b[39;49mparameters(), lr\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m~/Documents/Winter_Thesis/Coding_Learning/Master-Thesis/.venvn/lib/python3.9/site-packages/torch/optim/adam.py:18\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, params, lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m, betas\u001b[39m=\u001b[39m(\u001b[39m0.9\u001b[39m, \u001b[39m0.999\u001b[39m), eps\u001b[39m=\u001b[39m\u001b[39m1e-8\u001b[39m,\n\u001b[1;32m     15\u001b[0m              weight_decay\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, amsgrad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m, foreach: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m              maximize: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, capturable: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m              differentiable: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, fused: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 18\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m0.0\u001b[39;49m \u001b[39m<\u001b[39;49m\u001b[39m=\u001b[39;49m lr:\n\u001b[1;32m     19\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid learning rate: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(lr))\n\u001b[1;32m     20\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m0.0\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m eps:\n",
      "\u001b[0;31mTypeError\u001b[0m: '<=' not supported between instances of 'float' and 'str'"
     ]
    }
   ],
   "source": [
    "Optimizer = optim.Adam(model_latlon_AOD_PM25.parameters(), lr=float(config[\"train\"][\"lr\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0a48ba-5af7-4470-b6cb-7121204e6813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1e-5'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(config[\"train\"][\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb7cae9-aef9-4168-8932-f71ec6830dd0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '1e-5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m/Users/pavankumar/Documents/Winter_Thesis/Coding_Learning/Master-Thesis/src/training/trainer_latlon_AOD_PM25.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mint\u001b[39;49m(config[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '1e-5'"
     ]
    }
   ],
   "source": [
    "int(float(config[\"train\"][\"lr\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0fcfc5-c4aa-4319-b63b-a36861e803ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-05"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(float(config[\"train\"][\"lr\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvn (3.9.22)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
